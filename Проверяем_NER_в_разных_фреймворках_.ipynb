{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Проверяем NER в разных фреймворках.",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MutuLawyer/MutuPublic/blob/master/%D0%9F%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D1%8F%D0%B5%D0%BC_NER_%D0%B2_%D1%80%D0%B0%D0%B7%D0%BD%D1%8B%D1%85_%D1%84%D1%80%D0%B5%D0%B9%D0%BC%D0%B2%D0%BE%D1%80%D0%BA%D0%B0%D1%85_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbEbL4dNHcA9",
        "colab_type": "text"
      },
      "source": [
        "# NER frameworks examples with MITLABS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFeQ3nKCPAis",
        "colab_type": "text"
      },
      "source": [
        "##Set an example text for parsing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BINfboLPqcO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#Set an example text for parsing \n",
        "english_text = ''' I want a person available 7 days and with prompt response all most every time. Only Indian freelancer need I need PHP developer who have strong experience in Laravel and Codeigniter framework for daily 4 hours. I need this work by Monday 27th Jan. should be free from plagiarism . \n",
        "Need SAP FICO consultant for support project needs to be work on 6 months on FI AREAWe.  Want a same site to be created as the same as this https://www.facebook.com/?ref=logo, please check the site before contacting to me and i want this site to be ready in 10 days. They will be ready at noon tomorrow .'''\n",
        "\n",
        "russian_text = '''Власти Москвы выделили 110 млрд рублей на поддержку населения, системы здравоохранения и городского хозяйства. Об этом сообщается на сайте мэра столицы https://www.sobyanin.ru/ в пятницу, 1 мая. По адресу Алтуфьевское шоссе д.51 (основной вид разрешенного использования: производственная деятельность, склады) размещен МПЗ? Подпоручик Киже управляя автомобилем ВАЗ2107 перевозил автомат АК47 с целью ограбления банка ВТБ24, как следует из записей. \n",
        "Взыскать c индивидуального предпринимателя Иванова Костантипа Петровича дата рождения 10 января 1970 года, проживающего по адресу город Санкт-Петербург, ул. Крузенштерна, дом 5/1А 8 000 (восемь тысяч) рублей 00 копеей госпошлины в пользу бюджета РФ Жители требуют незамедлительной остановки МПЗ и его вывода из района. Решение было принято по поручению мэра города Сергея Собянина в связи с ограничениями из-за коронавируса.'''\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdHi9MLEt6OB",
        "colab_type": "text"
      },
      "source": [
        "## NLTK\n",
        "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
        "\n",
        "[link](https://www.nltk.org/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLtIbPYJuAfe",
        "colab_type": "code",
        "outputId": "20fb65fd-d725-4def-b438-140ad8f9bada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErxDoYwdRocK",
        "colab_type": "code",
        "outputId": "227720f3-18b4-4d9d-af49-fc8917eb73b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "for sent in nltk.sent_tokenize(english_text):\n",
        "   for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "      if hasattr(chunk, 'label'):\n",
        "         print(chunk)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(GPE New/NNP York/NNP)\n",
            "(GPE Indian/JJ)\n",
            "(ORGANIZATION PHP/NNP)\n",
            "(GPE Laravel/NNP)\n",
            "(PERSON Mona/NNP Lisa/NNP)\n",
            "(PERSON Need/NNP)\n",
            "(ORGANIZATION SAP/NNP)\n",
            "(ORGANIZATION FI/NNP)\n",
            "(GPE Iceland/NNP)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjg7Sj3Tt8bz",
        "colab_type": "text"
      },
      "source": [
        "## Spacy\n",
        "![alt text](https://spacy.io/static/social_default-1d3b50b1eba4c2b06244425ff0c49570.jpg)\n",
        "\n",
        "spaCy is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. spaCy comes with pretrained statistical models and word vectors, and currently supports tokenization for 50+ languages. It features state-of-the-art speed, convolutional neural network models for tagging, parsing and named entity recognition and easy deep learning integration. It's commercial open-source software, released under the MIT license.\n",
        "\n",
        "[link](https://spacy.io/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbk-G8JOx3nt",
        "colab_type": "code",
        "outputId": "8e62ad9a-d0ce-4ff5-de52-dff3a42b77c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "!python3 -m spacy download en_core_web_lg\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=ac8619d2a53364180ee9715062047fea0e5cf5a4f15ca8923a1c6dd9454b86e6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fkz_6eu1/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AklqU1X4ANx8",
        "colab_type": "text"
      },
      "source": [
        "Please, restart runtime!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfscJhy77neC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_text = '''I want a person available 7 days and with prompt response all most every time in New York. Only Indian freelancer need I need PHP developer who have strong experience in Laravel and Codeigniter framework for daily 4 hours. I need this work by Monday 27th Jan. Should be free from plagiarism like Mona Lisa. \n",
        "Need SAP FICO consultant for support project needs to be work on 6 months on FI AREAWe.  Want a same site to be created as the same as this https://www.facebook.com/?ref=logo, please check the site before contacting to me and i want this site to be ready in 10 days. The first plane will be ready at noon tomorrow in Iceland.'''\n",
        "\n",
        "\n",
        "russian_text = '''Власти Москвы выделили 110 млрд рублей на поддержку населения, системы здравоохранения и городского хозяйства. Об этом сообщается на сайте мэра столицы https://www.sobyanin.ru/ в пятницу, 1 мая. По адресу Алтуфьевское шоссе д.51 (основной вид разрешенного использования: производственная деятельность, склады) размещен МПЗ\n",
        "Взыскать к индивидуального предпринимателя Иванова Костантипа Петровича дата рождения 10 января 1970 года, проживающего по адресу город Санкт-Петербург, ул. Крузенштерна, дом 5/1А 8 000 (восемь тысяч) рублей 00 копеей госпошлины в пользу бюджета РФ Жители требуют незамедлительной остановки МПЗ и его вывода из района. Решение было принято по поручению мэра города Сергея Собянина в связи с ограничениями из-за коронавируса.'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkg1NC08xVYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import en_core_web_lg\n",
        "import spacy\n",
        "model_sp = en_core_web_lg.load()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghnQyFifqqeX",
        "colab_type": "code",
        "outputId": "bcea9fab-5635-4e8e-d112-b9159676e1c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "for ent in model_sp(english_text).ents:\n",
        "  print(ent.text.strip(), ent.label_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7 days DATE\n",
            "Indian NORP\n",
            "Laravel LOC\n",
            "Codeigniter NORP\n",
            "4 hours TIME\n",
            "Monday 27th Jan. DATE\n",
            "FICO ORG\n",
            "6 months DATE\n",
            "10 days DATE\n",
            "noon TIME\n",
            "tomorrow DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy5wwTpCuutS",
        "colab_type": "text"
      },
      "source": [
        "## Flair\n",
        "![alt text](https://raw.githubusercontent.com/flairNLP/flair/master/resources/docs/flair_logo_2020.png)\n",
        "\n",
        "Flair is:\n",
        "\n",
        "*   A powerful NLP library. Flair allows you to apply our state-of-the-art natural language processing (NLP) models to your text, such as named entity recognition (NER), part-of-speech tagging (PoS), sense disambiguation and classification.\n",
        "*   Multilingual. Thanks to the Flair community, we support a rapidly growing number of languages. We also now include 'one model, many languages' taggers, i.e. single models that predict PoS or NER tags for input text in various languages.\n",
        "\n",
        "*   A text embedding library. Flair has simple interfaces that allow you to use and combine different word and document embeddings, including our proposed Flair embeddings, BERT embeddings and ELMo embeddings.\n",
        "\n",
        "*   A PyTorch NLP framework. Our framework builds directly on PyTorch, making it easy to train your own models and experiment with new approaches using Flair embeddings and classes.\n",
        "\n",
        "[link](https://github.com/flairNLP/flair)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zkh_P6buwRq",
        "colab_type": "code",
        "outputId": "d1bf2980-293c-4c2f-8cab-c761f7fa316a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install --upgrade git+https://github.com/flairNLP/flair.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/flairNLP/flair.git\n",
            "  Cloning https://github.com/flairNLP/flair.git to /tmp/pip-req-build-be9pel0c\n",
            "  Running command git clone -q https://github.com/flairNLP/flair.git /tmp/pip-req-build-be9pel0c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.5.0+cu101)\n",
            "Collecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/f9/9f2b6c672c8f8bb87a4c1bd52c1b57213627b035305aad745d015b2a62ae/pytest-5.4.2-py3-none-any.whl (247kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (3.6.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.22.2.post1)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Collecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.8.7)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2.8.1)\n",
            "Collecting transformers>=2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 14.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2019.12.20)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 21.2MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair==0.4.5) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair==0.4.5) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (8.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (0.1.9)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (20.3)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (19.3.0)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (1.8.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.5) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.5) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.5) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.4.5) (0.14.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 32.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair==0.4.5) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (3.0.12)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 60.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.4.5) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair==0.4.5) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (1.13.4)\n",
            "Requirement already satisfied, skipping upgrade: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair==0.4.5) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair==0.4.5) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair==0.4.5) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair==0.4.5) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair==0.4.5) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.6.0->flair==0.4.5) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.9.5)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.15.2)\n",
            "Building wheels for collected packages: flair\n",
            "  Building wheel for flair (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flair: filename=flair-0.4.5-cp36-none-any.whl size=148247 sha256=365509cd27b2c696909e9f252edcb62d69ecb3c0623e5b79820c1950a4ee9f7f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dz9q148k/wheels/84/82/73/d2b3b59b7be74ea05f2c6d64132efe27df52daffb47d1dc7bb\n",
            "Successfully built flair\n",
            "Building wheels for collected packages: sqlitedict, mpld3, segtok, langdetect, sacremoses\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=62d903339316263a5438a15dbaaa7e95d9a9150eed1127c53aa742eb015125ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=90993a116f26d7ba0803702a7eb215bd761a1b8b327d349e1772069211cdebb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25020 sha256=c24f6e79b026562c615d07573db5720cbdcd8dc2695533b49e376b0daaae8ab8\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993193 sha256=f0d132bd09986615d10f1e7a9c95df6937313e7d42d9e1798a3fbaea2ceade67\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=6ed7bee13cad9c3634f13bd2acc7136e94ae9efa6bb1df787b71ff5af2827e28\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sqlitedict mpld3 segtok langdetect sacremoses\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pluggy, pytest, sqlitedict, sentencepiece, bpemb, mpld3, segtok, tokenizers, sacremoses, transformers, langdetect, deprecated, flair\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed bpemb-0.3.0 deprecated-1.2.10 flair-0.4.5 langdetect-1.0.8 mpld3-0.3 pluggy-0.13.1 pytest-5.4.2 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.90 sqlitedict-1.6.0 tokenizers-0.7.0 transformers-2.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbzepNzhzVkY",
        "colab_type": "code",
        "outputId": "f95276e5-f43e-411b-dc4b-015f0149b2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "from flair.models import SequenceTagger\n",
        "tagger = SequenceTagger.load('ner-ontonotes')\n",
        "from flair.data import Sentence\n",
        "s = Sentence(english_text)\n",
        "tagger.predict(s)\n",
        "for entity in s.get_spans('ner'):\n",
        "    print(entity)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-15 13:42:09,124 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/release-ner-ontonotes-0/en-ner-ontonotes-v0.4.pt not found in cache, downloading to /tmp/tmpjpn_nirk\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1510100570/1510100570 [00:56<00:00, 26672487.37B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-15 13:43:06,345 copying /tmp/tmpjpn_nirk to cache at /root/.flair/models/en-ner-ontonotes-v0.4.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-15 13:43:11,228 removing temp file /tmp/tmpjpn_nirk\n",
            "2020-05-15 13:43:11,361 loading file /root/.flair/models/en-ner-ontonotes-v0.4.pt\n",
            "Span [6,7]: \"7 days\"   [− Labels: DATE (0.9329)]\n",
            "Span [17]: \"Indian\"   [− Labels: NORP (0.9994)]\n",
            "Span [35,36]: \"4 hours.\"   [− Labels: TIME (0.7594)]\n",
            "Span [42,43,44]: \"Monday 27th Jan.\"   [− Labels: DATE (0.9109)]\n",
            "Span [53]: \"FICO\"   [− Labels: ORG (0.6987)]\n",
            "Span [63,64]: \"6 months\"   [− Labels: DATE (0.9412)]\n",
            "Span [98,99]: \"10 days.\"   [− Labels: DATE (0.9320)]\n",
            "Span [105,106]: \"noon tomorrow\"   [− Labels: TIME (0.8667)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wpu5FBVxnjL",
        "colab_type": "text"
      },
      "source": [
        "Please, make factory reset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOO_vNe32Ze6",
        "colab_type": "text"
      },
      "source": [
        "## Deep Pavlov\n",
        "![alt text](https://avatars3.githubusercontent.com/u/29918795?s=400&v=4)\n",
        "\n",
        "DeepPavlov is an open-source conversational AI library built on TensorFlow and Keras.\n",
        "\n",
        "DeepPavlov is designed for development of production ready chat-bots and complex conversational systems, research in the area of NLP and, particularly, of dialog systems.\n",
        "\n",
        "[link](http://deeppavlov.ai/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJjOphfgXmsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#Set an example text for parsing \n",
        "english_text = ''' I want a person available 7 days and with prompt response all most every time. Only Indian freelancer need I need PHP developer who have strong experience in Laravel and Codeigniter framework for daily 4 hours. I need this work by Monday 27th Jan. should be free from plagiarism . \n",
        "Need SAP FICO consultant for support project needs to be work on 6 months on FI AREAWe.  Want a same site to be created as the same as this https://www.facebook.com/?ref=logo, please check the site before contacting to me and i want this site to be ready in 10 days. They will be ready at noon tomorrow .'''\n",
        "\n",
        "russian_text = '''Власти Москвы выделили 110 млрд рублей на поддержку населения, системы здравоохранения и городского хозяйства. Об этом сообщается на сайте мэра столицы https://www.sobyanin.ru/ в пятницу, 1 мая. По адресу Алтуфьевское шоссе д.51 (основной вид разрешенного использования: производственная деятельность, склады) размещен МПЗ\n",
        "Взыскать к индивидуального предпринимателя Иванова Костантипа Петровича дата рождения 10 января 1970 года, проживающего по адресу город Санкт-Петербург, ул. Крузенштерна, дом 5/1А 8 000 (восемь тысяч) рублей 00 копеей госпошлины в пользу бюджета РФ Жители требуют незамедлительной остановки МПЗ и его вывода из района. Решение было принято по поручению мэра города Сергея Собянина в связи с ограничениями из-за коронавируса.'''\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xLK6na62sY1",
        "colab_type": "code",
        "outputId": "bd6745b2-f561-449c-ffa7-b9ee76453599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install deeppavlov\n",
        "!python3 -m deeppavlov install ner_ontonotes\n",
        "!python -m deeppavlov install ner_ontonotes_bert\n",
        "from deeppavlov import configs, build_model\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeppavlov\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/74/600871c188f82023575c86f6119804c35f0cdbd009bd5b73e4538978ba3d/deeppavlov-0.9.1-py3-none-any.whl (777kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.3MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml==0.15.100\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/9f/83bb34eaf84032b0b54fcc4a6aff1858572d279d65a301c7ae875f523df5/ruamel.yaml-0.15.100-cp36-cp36m-manylinux1_x86_64.whl (656kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 8.6MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.4.1)\n",
            "Collecting overrides==2.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n",
            "Collecting pydantic==1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/24/e78cf017628e7eaed20cb040999b1ecc69f872da53dfd0d9aed40c0fa5f1/pydantic-1.3-cp36-cp36m-manylinux2010_x86_64.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 9.5MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/45f71bd24f4e37629e9db5fb75caab919507deae6a5a257f9e4685a5f931/numpy-1.18.0-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 35.6MB/s \n",
            "\u001b[?25hCollecting pandas==0.25.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 33.5MB/s \n",
            "\u001b[?25hCollecting pyopenssl==19.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (4.41.1)\n",
            "Collecting Cython==0.29.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/d1/4d3f8a7a920e805488a966cc6ab55c978a712240f584445d703c08b9f405/Cython-0.29.14-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 31.3MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/04/49633f490f726da6e454fddc8e938bbb5bfed2001681118d3814c219b723/scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 32.1MB/s \n",
            "\u001b[?25hCollecting aio-pika==6.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/07/196a4115cbef31fa0c3dabdea146f02dffe5e49998341d20dbe2278953bc/aio_pika-6.4.1-py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0MB 35.5MB/s \n",
            "\u001b[?25hCollecting pytelegrambotapi==3.6.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/99c606f69fcda57e35788b913dd34c9d9acb48dd26349141b3855dcf6351/pyTelegramBotAPI-3.6.7.tar.gz (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.10.0)\n",
            "Collecting fastapi==0.47.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/a7/4804d7abf8a1544d079d50650af872387154ebdac5bd07d54b2e60e2b334/fastapi-0.47.1-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hCollecting fuzzywuzzy==0.17.0\n",
            "  Downloading https://files.pythonhosted.org/packages/d8/f1/5a267addb30ab7eaa1beab2b9323073815da4551076554ecc890a3595ec9/fuzzywuzzy-0.17.0-py2.py3-none-any.whl\n",
            "Collecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.6MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 50.9MB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/11f4b4bf3963ead6de570feeae49eeced02f6768cf1f68e16f4b16d3b0aa/uvicorn-0.11.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic==1.3->deeppavlov) (0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->deeppavlov) (2018.9)\n",
            "Collecting cryptography>=2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/04/686efee2dcdd25aecf357992e7d9362f443eb182ecd623f882bc9f7a6bba/cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 51.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from pyopenssl==19.1.0->deeppavlov) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.2->deeppavlov) (0.14.1)\n",
            "Collecting yarl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/8f/0209fc5d975f839344c33c822ff2f7ef80f6b1e984673a5a68f960bfa583/yarl-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 61.5MB/s \n",
            "\u001b[?25hCollecting aiormq<4,>=3.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/90/e8089608c2fcf75d6a8ff805611a1e31ed7cc523f5eafe244a2fb3dd75b8/aiormq-3.2.2-py3-none-any.whl\n",
            "Collecting starlette<=0.12.9,>=0.12.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/2220fe5bf287e693a6430d8ee36c681b0157035b7249ec08f8fb36319d16/starlette-0.12.9.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 39.7MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting websockets==8.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n",
            "\u001b[?25hCollecting httptools==0.0.13; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/03/215969db11abe8741e9c266a4cbe803a372bd86dd35fa0084c4df6d4bd00/httptools-0.0.13.tar.gz (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 61.3MB/s \n",
            "\u001b[?25hCollecting uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/48/586225bbb02d3bdca475b17e4be5ce5b3f09da2d6979f359916c1592a687/uvloop-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.* in /usr/local/lib/python3.6/dist-packages (from uvicorn==0.11.1->deeppavlov) (7.1.2)\n",
            "Collecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.0)\n",
            "Collecting multidict>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/2e/3ab2f1fb72571f75013db323a3799d505d99f3bc203513604f1ffb9b7858/multidict-4.7.5-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 40.0MB/s \n",
            "\u001b[?25hCollecting pamqp==2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/56/afa06143361e640c9159d828dadc95fc9195c52c95b4a97d136617b0166d/pamqp-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
            "Building wheels for collected packages: overrides, pytelegrambotapi, nltk, starlette, httptools\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-cp36-none-any.whl size=5600 sha256=c02d8c2ab8fa09c412e4945c790c85913f01af34d096a6b8a3306fa516894304\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-cp36-none-any.whl size=47178 sha256=04682d66fa792940007cd14f45ab653571ff8e121f6310deb7c2ec191b7b0382\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/40/18/8a34153f95ef0dc19e3954898e5a5079244b76a8afdd7d0ec5\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449906 sha256=d008c5c9e2e5b8bab5f85b912a073a1543e6ca659dbaf90520de8aaf3dac93d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for starlette: filename=starlette-0.12.9-cp36-none-any.whl size=57245 sha256=6b40c396e913e4dcdbb0511809e9ec469730a4a9e0170f4ea68670c2c5c9bd5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5\n",
            "  Building wheel for httptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for httptools: filename=httptools-0.0.13-cp36-cp36m-linux_x86_64.whl size=212531 sha256=d4851c1a300229318f35fb0a2b21f18aad7179c3986bd93a5a4c8b92eb771c68\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/3e/2e/013f99b42efc25cf3589730cf380738e46b1e5edaf2f78d525\n",
            "Successfully built overrides pytelegrambotapi nltk starlette httptools\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: ruamel.yaml, rusenttokenize, overrides, pydantic, numpy, pandas, cryptography, pyopenssl, Cython, scikit-learn, idna, multidict, yarl, pamqp, aiormq, aio-pika, pymorphy2-dicts-ru, requests, pytelegrambotapi, starlette, fastapi, fuzzywuzzy, nltk, pymorphy2-dicts, dawg-python, pymorphy2, websockets, httptools, uvloop, h11, uvicorn, deeppavlov\n",
            "  Found existing installation: numpy 1.18.4\n",
            "    Uninstalling numpy-1.18.4:\n",
            "      Successfully uninstalled numpy-1.18.4\n",
            "  Found existing installation: pandas 1.0.3\n",
            "    Uninstalling pandas-1.0.3:\n",
            "      Successfully uninstalled pandas-1.0.3\n",
            "  Found existing installation: Cython 0.29.17\n",
            "    Uninstalling Cython-0.29.17:\n",
            "      Successfully uninstalled Cython-0.29.17\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: idna 2.9\n",
            "    Uninstalling idna-2.9:\n",
            "      Successfully uninstalled idna-2.9\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.2.2 cryptography-2.9.2 dawg-python-0.7.2 deeppavlov-0.9.1 fastapi-0.47.1 fuzzywuzzy-0.17.0 h11-0.9.0 httptools-0.0.13 idna-2.8 multidict-4.7.5 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.404381.4453942 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 scikit-learn-0.21.2 starlette-0.12.9 uvicorn-0.11.1 uvloop-0.14.0 websockets-8.1 yarl-1.4.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "numpy",
                  "pandas",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-14 18:08:31.923 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ner_ontonotes' as '/usr/local/lib/python3.6/dist-packages/deeppavlov/configs/ner/ner_ontonotes.json'\n",
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (1.18.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (2.0.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.1) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.1) (1.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.1) (2.22.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.1) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.1) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.1) (1.16.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->smart-open>=1.8.1->gensim==3.8.1) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->smart-open>=1.8.1->gensim==3.8.1) (0.15.2)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.1\n",
            "Collecting tensorflow==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d9/fd234c7bf68638423fb8e7f44af7fcfce3bcaf416b51e6d902391e47ec43/tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.34.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.18.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.10.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 55.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.28.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 52.7MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (46.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=e5e690515a46d9bd8d64da0d7f2e064de5a35942da82e2cb3688ff1500a531bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0rc0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n",
            "2020-05-14 18:09:47.500 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ner_ontonotes_bert' as '/usr/local/lib/python3.6/dist-packages/deeppavlov/configs/ner/ner_ontonotes_bert.json'\n",
            "Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.6/dist-packages (1.15.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.28.1)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.18.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (46.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
            "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-0xs2yd2q\n",
            "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-0xs2yd2q\n",
            "  Running command git checkout -b feat/multi_gpu --track origin/feat/multi_gpu\n",
            "  Switched to a new branch 'feat/multi_gpu'\n",
            "  Branch 'feat/multi_gpu' set up to track remote branch 'feat/multi_gpu' from 'origin'.\n",
            "Building wheels for collected packages: bert-dp\n",
            "  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-dp: filename=bert_dp-1.0-cp36-none-any.whl size=23581 sha256=41a3cf203f5c9122695a9c679a770d5800f367636c7372de1d445433ae6579c8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9s7nsyg8/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n",
            "Successfully built bert-dp\n",
            "Installing collected packages: bert-dp\n",
            "Successfully installed bert-dp-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6bRJg429Cfe",
        "colab_type": "code",
        "outputId": "e69bc97b-172b-42b0-9f69-7e7a9b551acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from deeppavlov import build_model, configs\n",
        "\n",
        "ner_model = build_model(configs.ner.ner_ontonotes_bert, download=True)\n",
        "result = ner_model([english_text])\n",
        "for i in range(len(result[0][0])):\n",
        "     if result [1][0][i] != 'O':\n",
        "         print(result[0][0][i], result[1][0][i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-14 18:09:57.91 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/deeppavlov_data/ner_ontonotes_bert_v1.tar.gz to /root/.deeppavlov/ner_ontonotes_bert_v1.tar.gz\n",
            "100%|██████████| 805M/805M [03:23<00:00, 3.95MB/s]\n",
            "2020-05-14 18:13:20.876 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting /root/.deeppavlov/ner_ontonotes_bert_v1.tar.gz archive into /root/.deeppavlov/models\n",
            "2020-05-14 18:13:30.4 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip to /root/.deeppavlov/downloads/cased_L-12_H-768_A-12.zip\n",
            "100%|██████████| 404M/404M [01:18<00:00, 5.16MB/s]\n",
            "2020-05-14 18:14:48.344 INFO in 'deeppavlov.core.data.utils'['utils'] at line 242: Extracting /root/.deeppavlov/downloads/cased_L-12_H-768_A-12.zip archive into /root/.deeppavlov/downloads/bert_models\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-14 18:14:55.160 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_ontonotes_bert/tag.dict]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:236: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:314: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n",
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:75: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:94: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:671: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:249: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-14 18:15:20.551 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/ner_ontonotes_bert/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/ner_ontonotes_bert/model\n",
            "7 B-DATE\n",
            "days I-DATE\n",
            "Indian B-NORP\n",
            "Laravel B-PRODUCT\n",
            "Codeigniter B-PRODUCT\n",
            "daily B-DATE\n",
            "4 B-TIME\n",
            "hours I-TIME\n",
            "Monday B-DATE\n",
            "27th I-DATE\n",
            "Jan I-DATE\n",
            "6 B-DATE\n",
            "months I-DATE\n",
            "FI B-PRODUCT\n",
            "AREAWe I-PRODUCT\n",
            "10 B-DATE\n",
            "days I-DATE\n",
            "noon B-TIME\n",
            "tomorrow B-DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBqxcYavqoah",
        "colab_type": "text"
      },
      "source": [
        "## Stanford Core NLP\n",
        "\n",
        "![alt text](https://images.g2crowd.com/uploads/product/image/social_landscape/social_landscape_8c3334e07be0d50b757d5526a9acebcc/stanford-corenlp.jpg)\n",
        "\n",
        "Stanford CoreNLP provides a set of human language technology tools. It can give the base forms of words, their parts of speech, whether they are names of companies, people, etc., normalize dates, times, and numeric quantities, mark up the structure of sentences in terms of phrases and syntactic dependencies, indicate which noun phrases refer to the same entities, indicate sentiment, extract particular or open-class relations between entity mentions, get the quotes people said, etc.\n",
        "\n",
        "Choose Stanford CoreNLP if you need:\n",
        "\n",
        "* An integrated NLP toolkit with a broad range of grammatical analysis tools\n",
        "* A fast, robust annotator for arbitrary texts, widely used in production\n",
        "* A modern, regularly updated package, with the overall highest quality text analytics\n",
        "* Support for a number of major (human) languages\n",
        "* Available APIs for most major modern programming languages\n",
        "* Ability to run as a simple web service\n",
        "\n",
        "[link](https://stanfordnlp.github.io/CoreNLP/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6in8psMKLMs",
        "colab_type": "code",
        "outputId": "7bd80276-193b-4cb7-b1a4-618424918d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "!pip3 install nltk==3.2.4\n",
        "!wget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\n",
        "!unzip stanford-ner-2015-04-20.zip\n",
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "jar = \"stanford-ner-2015-04-20/stanford-ner-3.5.2.jar\"\n",
        "model = \"stanford-ner-2015-04-20/classifiers/\" \n",
        "st_3class = StanfordNERTagger(model + \"english.all.3class.distsim.crf.ser.gz\", jar, encoding='utf8') \n",
        "st_4class = StanfordNERTagger(model + \"english.conll.4class.distsim.crf.ser.gz\", jar, encoding='utf8') \n",
        "st_7class = StanfordNERTagger(model + \"english.muc.7class.distsim.crf.ser.gz\", jar, encoding='utf8')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.2.4 in /usr/local/lib/python3.6/dist-packages (3.2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.2.4) (1.12.0)\n",
            "--2020-05-14 18:21:16--  http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip [following]\n",
            "--2020-05-14 18:21:16--  https://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 176961718 (169M) [application/zip]\n",
            "Saving to: ‘stanford-ner-2015-04-20.zip.1’\n",
            "\n",
            "stanford-ner-2015-0 100%[===================>] 168.76M  4.05MB/s    in 27s     \n",
            "\n",
            "2020-05-14 18:21:44 (6.20 MB/s) - ‘stanford-ner-2015-04-20.zip.1’ saved [176961718/176961718]\n",
            "\n",
            "Archive:  stanford-ner-2015-04-20.zip\n",
            "replace stanford-ner-2015-04-20/README.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: stanford-ner-2015-04-20/README.txt  \n",
            "  inflating: stanford-ner-2015-04-20/ner-gui.bat  \n",
            "  inflating: stanford-ner-2015-04-20/build.xml  \n",
            "  inflating: stanford-ner-2015-04-20/stanford-ner-3.5.2.jar  \n",
            "  inflating: stanford-ner-2015-04-20/stanford-ner.jar  \n",
            "  inflating: stanford-ner-2015-04-20/sample-conll-file.txt  \n",
            "  inflating: stanford-ner-2015-04-20/sample.ner.txt  \n",
            "  inflating: stanford-ner-2015-04-20/lib/jollyday-0.4.7.jar  \n",
            "  inflating: stanford-ner-2015-04-20/lib/joda-time.jar  \n",
            "  inflating: stanford-ner-2015-04-20/lib/stanford-ner-resources.jar  \n",
            "  inflating: stanford-ner-2015-04-20/ner-gui.command  \n",
            "  inflating: stanford-ner-2015-04-20/ner.sh  \n",
            "  inflating: stanford-ner-2015-04-20/NERDemo.java  \n",
            "  inflating: stanford-ner-2015-04-20/ner.bat  \n",
            "  inflating: stanford-ner-2015-04-20/stanford-ner-3.5.2-javadoc.jar  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.conll.4class.distsim.prop  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/example.serialized.ncc.ncc.ser.gz  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.muc.7class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.conll.4class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.muc.7class.distsim.prop  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.all.3class.distsim.prop  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/example.serialized.ncc.prop  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.all.3class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2015-04-20/stanford-ner-3.5.2-sources.jar  \n",
            "  inflating: stanford-ner-2015-04-20/sample.txt  \n",
            "  inflating: stanford-ner-2015-04-20/sample-w-time.txt  \n",
            "  inflating: stanford-ner-2015-04-20/ner-gui.sh  \n",
            "  inflating: stanford-ner-2015-04-20/LICENSE.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa2BHJJpQtOI",
        "colab_type": "code",
        "outputId": "74b15f71-2c26-462f-e761-5fa00f533279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "for i in [st_3class.tag(english_text.split()), st_4class.tag(english_text.split()), st_7class.tag(english_text.split())]:\n",
        "  for b in i:\n",
        "    if b[1] != 'O':\n",
        "        print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('PHP', 'ORGANIZATION')\n",
            "('Laravel', 'LOCATION')\n",
            "('Indian', 'MISC')\n",
            "('PHP', 'ORGANIZATION')\n",
            "('Laravel', 'LOCATION')\n",
            "('Codeigniter', 'PERSON')\n",
            "('SAP', 'ORGANIZATION')\n",
            "('FICO', 'ORGANIZATION')\n",
            "('PHP', 'ORGANIZATION')\n",
            "('Laravel', 'LOCATION')\n",
            "('Monday', 'DATE')\n",
            "('27th', 'DATE')\n",
            "('Jan.', 'DATE')\n",
            "('SAP', 'ORGANIZATION')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2uvc2hb1zrI",
        "colab_type": "text"
      },
      "source": [
        "## Polyglot\n",
        "\n",
        "Polyglot is a natural language pipeline that supports massive multilingual applications.\n",
        "\n",
        "[link](https://github.com/aboSamoor/polyglot)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ixzM5Kn12Ae",
        "colab_type": "code",
        "outputId": "fb94b6d7-d4df-4b93-9519-ba46767bcf26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "!pip3 install -U git+https://github.com/aboSamoor/polyglot.git@master\n",
        "!polyglot download embeddings2.en ner2.en\n",
        "from polyglot.text import Text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/aboSamoor/polyglot.git@master\n",
            "  Cloning https://github.com/aboSamoor/polyglot.git (to revision master) to /tmp/pip-req-build-bvwadvx9\n",
            "  Running command git clone -q https://github.com/aboSamoor/polyglot.git /tmp/pip-req-build-bvwadvx9\n",
            "Collecting morfessor>=2.0.2a1\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Collecting pycld2>=0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/d2/8b0def84a53c88d0eb27c67b05269fbd16ad68df8c78849e7b5d65e6aec3/pycld2-0.41.tar.gz (41.4MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4MB 103kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.7.3 in /usr/local/lib/python3.6/dist-packages (from polyglot==16.7.4) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from polyglot==16.7.4) (1.18.4)\n",
            "Collecting PyICU>=1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/b2/66a58057a537527d7307576f2d32f239cc411b911401276d6922caa94755/PyICU-2.4.3.tar.gz (219kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 57.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from polyglot==16.7.4) (0.34.2)\n",
            "Collecting futures>=2.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/05/80/f41cca0ea1ff69bce7e7a7d76182b47bb4e1a494380a532af3e8ee70b9ec/futures-3.1.1-py3-none-any.whl\n",
            "Building wheels for collected packages: polyglot, pycld2, PyICU\n",
            "  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=70643 sha256=b8a31a2247bcd2bccab73b68152b5e9fd2698b5f749ff2c6ae7cdb3bc24525d4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-igaracj1/wheels/42/d9/73/345c7ae8554299ff8b31635d64eb8455fd591385fa734cdbef\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp36-cp36m-linux_x86_64.whl size=9833501 sha256=ca645c904c125dbc41341827f21f7f8c83fcf953b70637ec6949f49ca2e482de\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/8f/e9/08a1a8932a490175bd140206cd86a3dbcfc70498100de11079\n",
            "  Building wheel for PyICU (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyICU: filename=PyICU-2.4.3-cp36-cp36m-linux_x86_64.whl size=1246990 sha256=beb3becf85c5a177f40b8e2a8da85926597e6e255725037627b419321040b980\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/14/f9/1d75d2d4be2e0169d986ccde156e25d9c772c91602dff3acc9\n",
            "Successfully built polyglot pycld2 PyICU\n",
            "Installing collected packages: morfessor, pycld2, PyICU, futures, polyglot\n",
            "Successfully installed PyICU-2.4.3 futures-3.1.1 morfessor-2.0.6 polyglot-16.7.4 pycld2-0.41\n",
            "[polyglot_data] Downloading package embeddings2.en to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data] Downloading package ner2.en to /root/polyglot_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzUrNQKk2X7O",
        "colab_type": "code",
        "outputId": "515a7267-ea0d-4291-c5ee-d698279b0629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "for ent in Text(english_text).entities:\n",
        " print(ent[0],ent.tag)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Laravel I-LOC\n",
            "SAP I-ORG\n",
            "FI I-ORG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K1dquH1EadO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "ce50ae2f-9e63-44b4-d0d9-82dd03d82ca8"
      },
      "source": [
        "!polyglot download embeddings2.ru ner2.ru\n",
        "for ent in Text(russian_text).entities:\n",
        " print(ent[0],ent.tag)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[polyglot_data] Downloading package embeddings2.ru to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data] Downloading package ner2.ru to /root/polyglot_data...\n",
            "ВТБ24 I-ORG\n",
            "Иванова I-PER\n",
            "Санкт I-LOC\n",
            "Крузенштерна I-PER\n",
            "РФ I-ORG\n",
            "Сергея I-PER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-2CQrYdcLQd",
        "colab_type": "text"
      },
      "source": [
        "**Factory reset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLZYMIshBmpt",
        "colab_type": "text"
      },
      "source": [
        "## Adapt NLP \n",
        "![alt text](https://raw.githubusercontent.com/novetta/adaptnlp/master/docs/img/NovettaAdaptNLPlogo-400px.png)\n",
        "\n",
        "A high level framework and library for running, training, and deploying state-of-the-art Natural Language Processing (NLP) models for end to end tasks.\n",
        "\n",
        "AdaptNLP allows users ranging from beginner python coders to experienced machine learning engineers to leverage state-of-the-art NLP models and training techniques in one easy-to-use python package.\n",
        "\n",
        "Built atop Zalando Research's Flair and Hugging Face's Transformers library, AdaptNLP provides Machine Learning Researchers and Scientists a modular and adaptive approach to a variety of NLP tasks with an Easy API for training, inference, and deploying NLP-based microservices.\n",
        "\n",
        "[link](https://github.com/Novetta/adaptnlp)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gclbLxVFCg-c",
        "colab_type": "code",
        "outputId": "4057de35-f52f-40a6-9636-b6b5b8c5ee1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install adaptnlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adaptnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/eb/9bdafff0b245cd6ffaa06550ac64fc85a558fed1bbf8b821862e6044da1f/adaptnlp-0.1.6-py3-none-any.whl (72kB)\n",
            "\r\u001b[K     |████▌                           | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 30kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (7.1.2)\n",
            "Collecting html-text\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/8d/e43d6dcec7432732ac1955b73dcccb536d09211af343de0f1c8eb892df45/html_text-0.5.1-py2.py3-none-any.whl\n",
            "Collecting flair==0.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/29/81e3c9a829ec50857c23d82560941625f6b42ce76ee7c56ea9529e959d18/flair-0.4.5-py3-none-any.whl (136kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 10.3MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 17kB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/transformers/\u001b[0m\n",
            "\u001b[?25hCollecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 54.8MB/s \n",
            "\u001b[?25hCollecting jupyterlab\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/30/03638fbb348e55af6375916962ddbfca786bd31cff9899b86162e2fc0cda/jupyterlab-2.1.2-py3-none-any.whl (7.8MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8MB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (4.7.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (5.2.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (7.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from html-text->adaptnlp) (4.2.6)\n",
            "Collecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (0.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (4.41.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (2.8.1)\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 51.6MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 55.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (0.22.2.post1)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (3.6.0)\n",
            "Collecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/f9/9f2b6c672c8f8bb87a4c1bd52c1b57213627b035305aad745d015b2a62ae/pytest-5.4.2-py3-none-any.whl (247kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (3.2.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (0.8.7)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->adaptnlp) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->adaptnlp) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->adaptnlp) (1.18.4)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 52.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->adaptnlp) (1.13.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->adaptnlp) (3.0.12)\n",
            "Collecting jupyterlab-server<2.0,>=1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/65/c3/87504f2803277a2b609dcd27a0b4f2fc65f1bad181bc9938916847731f29/jupyterlab_server-1.1.3-py3-none-any.whl\n",
            "Requirement already satisfied: tornado!=6.0.0,!=6.0.1,!=6.0.2 in /usr/local/lib/python3.6/dist-packages (from jupyterlab->adaptnlp) (4.5.3)\n",
            "Requirement already satisfied: jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from jupyterlab->adaptnlp) (2.11.2)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (5.3.4)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (4.3.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (2.1.3)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (19.0.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (4.6.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (1.4.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.3)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (5.0.6)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (3.1.5)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->adaptnlp) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->adaptnlp) (1.0.18)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->adaptnlp) (0.8.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->adaptnlp) (3.5.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5->adaptnlp) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5->adaptnlp) (2.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5->adaptnlp) (3.10.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5->adaptnlp) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5->adaptnlp) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.4.5->adaptnlp) (0.14.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.4.5->adaptnlp) (1.12.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.5->adaptnlp) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (1.6.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (8.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (0.1.9)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (19.3.0)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (1.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (20.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5->adaptnlp) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5->adaptnlp) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5->adaptnlp) (1.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0->adaptnlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0->adaptnlp) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0->adaptnlp) (2.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0->adaptnlp) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0->adaptnlp) (1.16.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0->adaptnlp) (0.3.3)\n",
            "Collecting jsonschema>=3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
            "\u001b[?25hCollecting json5\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/4b/c0b4c7e238a98165a0281d6ad52ee4a8401318580d2fc9d3844dda2ef5f7/json5-0.9.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.10->jupyterlab->adaptnlp) (1.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets->qtconsole->jupyter->adaptnlp) (4.4.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->adaptnlp) (0.5.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->adaptnlp) (46.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->adaptnlp) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->adaptnlp) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->adaptnlp) (4.8.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->adaptnlp) (0.6.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5->adaptnlp) (2.49.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair==0.4.5->adaptnlp) (3.1.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers==2.8.0->adaptnlp) (0.15.2)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->jupyterlab-server<2.0,>=1.1.0->jupyterlab->adaptnlp) (0.16.0)\n",
            "Building wheels for collected packages: sqlitedict, mpld3, langdetect, segtok, sacremoses\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=05017649cac3608f2aa4e3207f81dc99c8cdcb1a34a4f48fd94c0eda34c2bebc\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=aa30eadf8bb1702b0c75a0ad4947e3089afc0e12b96731850e4cc14e2ca521f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993193 sha256=9269e03771fd5712fdd19bb765794aa91cfcd73fb74749f133d9ec94bdea2fa5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25020 sha256=2ce6570e221931a8c32ea38a837d3d5e7bb387276edc65ba3aabfa60a0d649a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=5af942995b68bacd4789caca38948497bf6c9867ed0b8a7ec8ae68f60e49987b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sqlitedict mpld3 langdetect segtok sacremoses\n",
            "\u001b[31mERROR: torchvision 0.6.0+cu101 has requirement torch==1.5.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html-text, sentencepiece, bpemb, sqlitedict, torch, mpld3, langdetect, deprecated, tokenizers, sacremoses, transformers, pluggy, pytest, segtok, flair, jsonschema, json5, jupyterlab-server, jupyterlab, adaptnlp\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "Successfully installed adaptnlp-0.1.6 bpemb-0.3.0 deprecated-1.2.10 flair-0.4.5 html-text-0.5.1 json5-0.9.4 jsonschema-3.2.0 jupyterlab-2.1.2 jupyterlab-server-1.1.3 langdetect-1.0.8 mpld3-0.3 pluggy-0.13.1 pytest-5.4.2 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.90 sqlitedict-1.6.0 tokenizers-0.5.2 torch-1.4.0 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_oyxA0GCTfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from adaptnlp import EasyTokenTagger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wwUNGjgBqZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tagger = EasyTokenTagger()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrXaQg1oBuAG",
        "colab_type": "code",
        "outputId": "61b8c911-8406-449e-bde4-324d4855cb32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "sentences = tagger.tag_text(\n",
        "    text = english_text, model_name_or_path = \"ner-ontonotes\"\n",
        ")\n",
        "spans = sentences[0].get_spans(\"ner\")\n",
        "for sen in sentences:\n",
        "    for entity in sen.get_spans(\"ner\"):\n",
        "        print(entity)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-15 08:57:23,230 loading file /root/.flair/models/en-ner-ontonotes-v0.4.pt\n",
            "DATE-span [6,7]: \"7 days\"\n",
            "NORP-span [18]: \"Indian\"\n",
            "PRODUCT-span [30]: \"Laravel\"\n",
            "TIME-span [35,36,37]: \"daily 4 hours\"\n",
            "DATE-span [44,45,46]: \"Monday 27th Jan.\"\n",
            "ORG-span [55]: \"FICO\"\n",
            "DATE-span [65,66]: \"6 months\"\n",
            "DATE-span [108,109]: \"10 days\"\n",
            "TIME-span [116,117]: \"noon tomorrow\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMGB79cty5nL",
        "colab_type": "text"
      },
      "source": [
        "## deepmipt/ner\n",
        "Several neural network architectures for named entity recognition from the paper \"Application of a Hybrid Bi-LSTM-CRF model to the task of Russian Named Entity Recognition\" https://arxiv.org/pdf/1709.09686.pdf, which is inspired by LSTM+CRF architecture from https://arxiv.org/pdf/1603.01360.pdf.\n",
        "\n",
        "NER class from ner/network.py provides methods for construction, training and inference neural networks for Named Entity Recognition.\n",
        "\n",
        "We provide pre-trained CNN model for Russian Named Entity Recognition. The model was trained on three datatasets:\n",
        "\n",
        "Gareev corpus [1] (obtainable by request to authors)\n",
        "FactRuEval 2016 [2]\n",
        "NE3 (extended Persons-1000) [3, 4]\n",
        "\n",
        "[link](https://github.com/deepmipt/ner)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps7uH30oy4nl",
        "colab_type": "code",
        "outputId": "6f86d581-ec6d-48d1-d2bf-5b80eec5bf30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install git+https://github.com/deepmipt/ner"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/deepmipt/ner\n",
            "  Cloning https://github.com/deepmipt/ner to /tmp/pip-req-build-dzpjk2aw\n",
            "  Running command git clone -q https://github.com/deepmipt/ner /tmp/pip-req-build-dzpjk2aw\n",
            "Collecting numpy==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/e2/57c1a6af4ff0ac095dd68b12bf07771813dbf401faf1b97f5fc0cb963647/numpy-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\n",
            "\u001b[K     |████████████████████████████████| 17.0MB 229kB/s \n",
            "\u001b[?25hCollecting tensorflow==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9f/57e1404fc9345759e4a732c4ab48ab4dd78fd1e60ee1270442b8850fa75f/tensorflow-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (43.5MB)\n",
            "\u001b[K     |████████████████████████████████| 43.6MB 100kB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts==2.4.393442.3710985\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 39.6MB/s \n",
            "\u001b[?25hCollecting tqdm==4.19.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/3c/341b4fa23cb3abc335207dba057c790f3bb329f6757e1fcd5d347bcf8308/tqdm-4.19.5-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hCollecting requests==2.18.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/df/50aa1999ab9bde74656c2919d9c0c085fd2b3775fd3eca826012bef76d8c/requests-2.18.4-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.4MB/s \n",
            "\u001b[?25hCollecting gensim==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/ed/fbbb2cc3f37a39cc4ff8e5f667374478fb852b384840aa7feb9608144290/gensim-2.3.0.tar.gz (17.2MB)\n",
            "\u001b[K     |████████████████████████████████| 17.2MB 265kB/s \n",
            "\u001b[?25hCollecting tensorflow-tensorboard<0.2.0,>=0.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/31/bb4111c3141d22bd7b2b553a26aa0c1863c86cb723919e5bd7847b3de4fc/tensorflow_tensorboard-0.1.8-py3-none-any.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 53.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.3.0->ner==0.0.1) (0.34.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.3.0->ner==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.3.0->ner==0.0.1) (3.10.0)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->ner==0.0.1) (0.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.18.4->ner==0.0.1) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.18.4->ner==0.0.1) (3.0.4)\n",
            "Collecting urllib3<1.23,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/cb/6965947c13a94236f6d4b8223e21beb4d576dc72e8130bd7880f600839b8/urllib3-1.22-py2.py3-none-any.whl (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 58.0MB/s \n",
            "\u001b[?25hCollecting idna<2.7,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/cc/6dd9a3869f15c2edfab863b992838277279ce92663d334df9ecf5106f5c6/idna-2.6-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==2.3.0->ner==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: smart_open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==2.3.0->ner==0.0.1) (2.0.0)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0->ner==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0->ner==0.0.1) (1.0.1)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.3.0->tensorflow==1.3.0->ner==0.0.1) (46.1.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart_open>=1.2.1->gensim==2.3.0->ner==0.0.1) (1.13.4)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart_open>=1.2.1->gensim==2.3.0->ner==0.0.1) (2.49.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart_open>=1.2.1->gensim==2.3.0->ner==0.0.1) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->smart_open>=1.2.1->gensim==2.3.0->ner==0.0.1) (1.16.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart_open>=1.2.1->gensim==2.3.0->ner==0.0.1) (0.9.5)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->smart_open>=1.2.1->gensim==2.3.0->ner==0.0.1) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->smart_open>=1.2.1->gensim==2.3.0->ner==0.0.1) (2.8.1)\n",
            "Building wheels for collected packages: ner, gensim, html5lib\n",
            "  Building wheel for ner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ner: filename=ner-0.0.1-cp36-none-any.whl size=22531 sha256=655a6096d3bca0251ce25d5e3015e6448b13050eb5f16bc689811762c2b05500\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-aiy_oy5g/wheels/46/f5/1c/0657f016f0e9725ee09f56dab547bd0bcb76fbbbc067a950ea\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim: filename=gensim-2.3.0-cp36-cp36m-linux_x86_64.whl size=6504631 sha256=8243be872e7c299e6bcc65c94467b8ca801ac5b1e25f0e1a93a8b7e837b5cab2\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/1f/86/63c886325bdffa379a7c91499bc9ea6317a4e4e0fc6e2ff1ce\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=1df4f20e748650af54a711c9dd01488c832606e5f444cec07453e249678a2d3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built ner gensim html5lib\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.2 has requirement numpy>=1.17, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0rc0 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-datasets 2.1.0 has requirement requests>=2.19.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.2.1 has requirement requests<3,>=2.21.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.10.1 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scipy 1.4.1 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pywavelets 1.1.1 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 0.14.1 has requirement numpy>=1.14, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.0.3 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: featuretools 0.4.1 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.0.1.post1 has requirement numpy>=1.16, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, html5lib, bleach, tensorflow-tensorboard, tensorflow, dawg-python, pymorphy2-dicts, pymorphy2, tqdm, urllib3, idna, requests, gensim, ner\n",
            "  Found existing installation: numpy 1.18.4\n",
            "    Uninstalling numpy-1.18.4:\n",
            "      Successfully uninstalled numpy-1.18.4\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.5\n",
            "    Uninstalling bleach-3.1.5:\n",
            "      Successfully uninstalled bleach-3.1.5\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: idna 2.9\n",
            "    Uninstalling idna-2.9:\n",
            "      Successfully uninstalled idna-2.9\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed bleach-1.5.0 dawg-python-0.7.2 gensim-2.3.0 html5lib-0.9999999 idna-2.6 ner-0.0.1 numpy-1.13.1 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 requests-2.18.4 tensorflow-1.3.0 tensorflow-tensorboard-0.1.8 tqdm-4.19.5 urllib3-1.22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "numpy",
                  "requests",
                  "tqdm",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqhoKZ7SzFXC",
        "colab_type": "code",
        "outputId": "edbcebd0-dcf4-45ae-acc8-2579ae1cf1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import ner\n",
        "example = russian_text\n",
        "def deepmint_ner(text):\n",
        "  extractor = ner.Extractor()\n",
        "  for m in extractor(text):\n",
        "     print(m)\n",
        "deepmint_ner(example)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading from http://lnsigo.mipt.ru/export/models/ner/ner_model_total_rus.tar.gz to /usr/local/lib/python3.6/dist-packages/ner/extractor/../model/ner_model_total_rus.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 44.3M/44.3M [00:07<00:00, 5.77MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /usr/local/lib/python3.6/dist-packages/ner/extractor/../model/ner_model_total_rus.tar.gz archive into /usr/local/lib/python3.6/dist-packages/ner/extractor/../model\n",
            "INFO:tensorflow:Restoring parameters from /usr/local/lib/python3.6/dist-packages/ner/extractor/../model/ner_model\n",
            "Match(tokens=[Token(span=(7, 13), text='Москвы')], span=Span(start=7, end=13), type='LOC')\n",
            "Match(tokens=[Token(span=(492, 499), text='Иванова')], span=Span(start=492, end=499), type='PER')\n",
            "Match(tokens=[Token(span=(511, 520), text='Петровича'), Token(span=(521, 525), text='дата')], span=Span(start=511, end=525), type='PER')\n",
            "Match(tokens=[Token(span=(591, 600), text='Петербург')], span=Span(start=591, end=600), type='LOC')\n",
            "Match(tokens=[Token(span=(814, 820), text='Сергея'), Token(span=(821, 829), text='Собянина')], span=Span(start=814, end=829), type='PER')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVwM-La02SPz",
        "colab_type": "text"
      },
      "source": [
        "## Stanza\n",
        "![alt text](https://raw.githubusercontent.com/stanfordnlp/stanza/dev/images/stanza-logo.png)\n",
        "Stanza is a Python natural language analysis package. It contains tools, which can be used in a pipeline, to convert a string containing human language text into lists of sentences and words, to generate base forms of those words, their parts of speech and morphological features, to give a syntactic structure dependency parse, and to recognize named entities. The toolkit is designed to be parallel among more than 70 languages, using the Universal Dependencies formalism.\n",
        "\n",
        "Stanza is built with highly accurate neural network components that also enable efficient training and evaluation with your own annotated data. The modules are built on top of the PyTorch library. You will get much faster performance if you run this system on a GPU-enabled machine.\n",
        "\n",
        "In addition, Stanza includes a Python interface to the CoreNLP Java package and inherits additonal functionality from there, such as constituency parsing, coreference resolution, and linguistic pattern matching.\n",
        "\n",
        "\n",
        "To summarize, Stanza features:\n",
        "\n",
        "Native Python implementation requiring minimal efforts to set up;\n",
        "Full neural network pipeline for robust text analytics, including tokenization, multi-word token (MWT) expansion, lemmatization, part-of-speech (POS) and morphological features tagging, dependency parsing, and named entity recognition;\n",
        "Pretrained neural models supporting 66 (human) languages;\n",
        "A stable, officially maintained Python interface to CoreNLP.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTSvmXHp2sHL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "2c30949b-d8f6-4a1e-8734-e10a331c782f"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/9c/60689521a971a57dd02d2925105efedefa9dccd76c9a0b92566683d43e89/stanza-1.0.1-py3-none-any.whl (193kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 24.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 81kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 92kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (46.1.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.12.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3bP5eO82Qsl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "431765ad-3999-47af-8a31-71a1c886832c"
      },
      "source": [
        "import stanza\n",
        "stanza.download('en')\n",
        "def stanza_nlp(text):\n",
        "  nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
        "  doc = nlp(text)\n",
        "  print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 11.9MB/s]                    \n",
            "2020-05-15 07:57:38 INFO: Downloading default packages for language: en (English)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/en/default.zip: 100%|██████████| 402M/402M [00:45<00:00, 8.76MB/s]\n",
            "2020-05-15 07:58:31 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLSJA0WR3eIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "014841a6-542f-4ea7-f368-dfe1cb10b9d0"
      },
      "source": [
        "stanza_nlp(english_text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-15 07:58:50 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | ewt       |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2020-05-15 07:58:50 INFO: Use device: cpu\n",
            "2020-05-15 07:58:50 INFO: Loading: tokenize\n",
            "2020-05-15 07:58:50 INFO: Loading: ner\n",
            "2020-05-15 07:58:50 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "entity: 7 days\ttype: DATE\n",
            "entity: Indian\ttype: NORP\n",
            "entity: Laravel\ttype: ORG\n",
            "entity: Codeigniter\ttype: PRODUCT\n",
            "entity: daily 4 hours\ttype: TIME\n",
            "entity: Monday 27th Jan.\ttype: DATE\n",
            "entity: SAP\ttype: ORG\n",
            "entity: FICO\ttype: ORG\n",
            "entity: 6 months\ttype: DATE\n",
            "entity: FI AREAWe\ttype: ORG\n",
            "entity: 10 days\ttype: DATE\n",
            "entity: noon tomorrow\ttype: TIME\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2QSfYRLCzkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "55fe7134-8783-487a-de93-56e9d7351aaa"
      },
      "source": [
        "import stanza\n",
        "stanza.download('ru')\n",
        "def stanza_nlp_ru(text):\n",
        "  nlp = stanza.Pipeline(lang='ru', processors='tokenize,ner')\n",
        "  doc = nlp(text)\n",
        "  print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 11.9MB/s]                    \n",
            "2020-05-15 07:59:01 INFO: Downloading default packages for language: ru (Russian)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/ru/default.zip: 100%|██████████| 591M/591M [01:38<00:00, 5.99MB/s]\n",
            "2020-05-15 08:00:49 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LYwRCoSDEZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "a5ad638f-e544-487e-c819-28cc64d4ea08"
      },
      "source": [
        "stanza_nlp_ru(russian_text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-15 08:01:18 INFO: Loading these models for language: ru (Russian):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | syntagrus |\n",
            "| ner       | wikiner   |\n",
            "=========================\n",
            "\n",
            "2020-05-15 08:01:18 INFO: Use device: cpu\n",
            "2020-05-15 08:01:18 INFO: Loading: tokenize\n",
            "2020-05-15 08:01:18 INFO: Loading: ner\n",
            "2020-05-15 08:01:19 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "entity: Москвы\ttype: LOC\n",
            "entity: Алтуфьевское шоссе\ttype: LOC\n",
            "entity: Киже\ttype: PER\n",
            "entity: ВАЗ2107\ttype: MISC\n",
            "entity: АК47\ttype: MISC\n",
            "entity: ВТБ24\ttype: MISC\n",
            "entity: Иванова Костантипа Петровича\ttype: PER\n",
            "entity: Санкт-Петербург\ttype: LOC\n",
            "entity: ул. Крузенштерна\ttype: LOC\n",
            "entity: РФ\ttype: LOC\n",
            "entity: МПЗ\ttype: LOC\n",
            "entity: Сергея Собянина\ttype: PER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1SwMQvMATe0",
        "colab_type": "text"
      },
      "source": [
        "##HanLP\n",
        "\n",
        "![alt text](https://repository-images.githubusercontent.com/24976755/b76ee080-2f44-11ea-9928-4e6109d3fd79)\n",
        "\n",
        "The multilingual NLP library for researchers and companies, built on TensorFlow 2.0, for advancing state-of-the-art deep learning techniques in both academia and industry. HanLP was designed from day one to be efficient, user friendly and extendable. It comes with pretrained models for various human languages including English, Chinese and many others. Currently, HanLP 2.0 is in alpha stage with more killer features on the roadmap. Discussions are welcomed on our forum, while bug reports and feature requests are reserved for GitHub issues. For Java users, please checkout the 1.x branch.\n",
        "\n",
        "[link](https://github.com/hankcs/HanLP/tree/master)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldoMzSX0AXQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "855eed21-943b-4c9c-c04e-964afce6e596"
      },
      "source": [
        "!pip install hanlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hanlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/9b/6ccc03d9460ff2f47598a45798dd95298ff66b54779e6c3909b62c967d9b/hanlp-2.0.0a44.tar.gz (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 3.4MB/s \n",
            "\u001b[?25hCollecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 21kB/s \n",
            "\u001b[?25hCollecting fasttext==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.9MB/s \n",
            "\u001b[?25hCollecting bert-for-tf2==0.12.7\n",
            "  Downloading https://files.pythonhosted.org/packages/93/31/1f9d1d5ccafb5b8bb621b02c4c5bd9e9f6599ec9b305f7307f1b6c5ae0b5/bert-for-tf2-0.12.7.tar.gz\n",
            "Collecting py-params==0.8.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ec/17/71c5f3c0ab511de96059358bcc5e00891a804cd4049021e5fa80540f201a/py-params-0.8.2.tar.gz\n",
            "Collecting params-flow==0.7.4\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/12/2604f88932f285a473015a5adabf08496d88dad0f9c1228fab1547ccc9b5/params-flow-0.7.4.tar.gz\n",
            "Collecting sentencepiece==0.1.85\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 57.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (1.4.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (0.8.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (1.18.4)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 55.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (0.9.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 58.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (1.28.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (3.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->hanlp) (3.10.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1->hanlp) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1->hanlp) (46.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow==0.7.4->hanlp) (4.41.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0->hanlp) (2.10.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (3.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->hanlp) (0.4.8)\n",
            "Building wheels for collected packages: hanlp, fasttext, bert-for-tf2, py-params, params-flow, gast\n",
            "  Building wheel for hanlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp: filename=hanlp-2.0.0a44-cp36-none-any.whl size=151714 sha256=c3cd32cdc1280584813fdd27567e3a6f729a7788000b0650a2e4c8ddd5d189a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/d0/25/5a2c332c57c75dd25c5dc7dc7a2bd6cdc1aeb7a91d1f62305a\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2388854 sha256=d39205cc186a3e52f4f68c69f5e7254155c667b818c6fb2f0f253db1f5ce920e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.12.7-cp36-none-any.whl size=29175 sha256=0b59d936dd02b9ec9939baf11921bc3d8cabedb6fc2b64878b56d50c7a537528\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/77/d0/2118abd9686bbeebfde72a494dfbdc012087e3560d9d380ab7\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.8.2-cp36-none-any.whl size=4635 sha256=cd7d5603b4d6d178f2c9aee6b993338be550f99bf3edd97ff781e98badcbd4ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/3a/9c/baf35d6f17f0c2c6b61bf8ac3ab9fc12df0e41432ccaeecacb\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.7.4-cp36-none-any.whl size=16195 sha256=11beb3a8680fb84b999e2e3b5747c7efd202750fc83e7491116f1bc7a1f69b5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/30/40/507b60d68b67ac87f35e95c98f5b296a32f146d5ae1d1d5aa7\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=b8ae0dcd47548c01591beafdef46e6e7e96fada0a33cd22e23e54cb1c1e3838e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built hanlp fasttext bert-for-tf2 py-params params-flow gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0rc0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorflow-estimator, tensorboard, tensorflow, fasttext, py-params, params-flow, bert-for-tf2, sentencepiece, hanlp\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed bert-for-tf2-0.12.7 fasttext-0.9.1 gast-0.2.2 hanlp-2.0.0a44 params-flow-0.7.4 py-params-0.8.2 sentencepiece-0.1.85 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaEjMNWHAeAI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "78df743e-957c-4665-e35b-ecb289feeaf7"
      },
      "source": [
        "import hanlp\n",
        "tokenizer = hanlp.utils.rules.tokenize_english\n",
        "testing = tokenizer('Need SAP FICO consultant for support project needs to be work on 6 months on FI AREAWe')\n",
        "recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)\n",
        "recognizer(testing)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Done loading 197 BERT weights from: /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7efe7d31e908> (prefix:bert_6). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n",
            "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op PaddedBatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op __inference_function_86145 in device /job:localhost/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('SAP FICO', 'ORG', 1, 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9jPDSY9Avpk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "783b9f64-897d-442d-e056-884b0e00c7d4"
      },
      "source": [
        "recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)\n",
        "recognizer([list('上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。'),\n",
        "                list('萨哈夫说，伊拉克将同联合国销毁伊拉克大规模杀伤性武器特别委员会继续保持合作。')])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip to /root/.hanlp/ner/ner_bert_base_msra_20200104_185735.zip\n",
            "100.00%, 360.3 MB/360.3 MB, 15.4 MB/s, ETA 0 s      \n",
            "Extracting /root/.hanlp/ner/ner_bert_base_msra_20200104_185735.zip to /root/.hanlp/ner\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip to /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
            "93.71%, 341.3 MB/364.2 MB, 14.8 MB/s, ETA 2 s      "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Done loading 197 BERT weights from: /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7ff05e3d5d30> (prefix:bert_1). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n",
            "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op PaddedBatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op __inference_function_20732 in device /job:localhost/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('上海华安工业（集团）公司', 'NT', 0, 12),\n",
              "  ('谭旭光', 'NR', 15, 18),\n",
              "  ('张晚霞', 'NR', 21, 24),\n",
              "  ('美国', 'NS', 26, 28),\n",
              "  ('纽约现代艺术博物馆', 'NS', 28, 37)],\n",
              " [('萨哈夫', 'NR', 0, 3),\n",
              "  ('伊拉克', 'NS', 5, 8),\n",
              "  ('联合国销毁伊拉克大规模杀伤性武器特别委员会', 'NT', 10, 31)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8CJgWQ9c30E",
        "colab_type": "text"
      },
      "source": [
        "## AllenNLP\n",
        "![alt text](https://raw.githubusercontent.com/allenai/allennlp/master/docs/img/allennlp-logo-dark.png)\n",
        "\n",
        "\n",
        "An Apache 2.0 NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNV3cNFjc3Sa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff696000-7166-4120-c9ed-f22c495aebcc"
      },
      "source": [
        "!pip install allennlp==1.0.0rc3 allennlp-models==1.0.0rc3\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.ner.crf_tagger\n",
        "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/ner-model-2020.02.10.tar.gz\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp==1.0.0rc3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/59/a93bca059fc9ef2331bdae71360a32959eddfe051c19e7e664588335cf86/allennlp-1.0.0rc3-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 3.3MB/s \n",
            "\u001b[?25hCollecting allennlp-models==1.0.0rc3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/3d/31892e9c5d0282e394c9a36112f12dc54b82ca85333480bbd6675f8179cb/allennlp_models-1.0.0rc3-py3-none-any.whl (271kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 64.8MB/s \n",
            "\u001b[?25hCollecting overrides==2.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/72/dd/ac49f9c69540d7e09210415801a05d0a54d4d0ca8401503c46847dacd3a0/overrides-2.8.0.tar.gz\n",
            "Collecting semantic-version\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/15/00ef3b7888a10363b7c402350eda3acf395ff05bebae312d1296e528516a/semantic_version-2.8.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: spacy<2.3,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==1.0.0rc3) (2.2.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp==1.0.0rc3) (1.13.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp==1.0.0rc3) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp==1.0.0rc3) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from allennlp==1.0.0rc3) (0.7)\n",
            "Collecting torch<=1.5.0,>1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/70/54e9fb010fe1547bc4774716f11ececb81ae5b306c05f090f4461ee13205/torch-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (752.0MB)\n",
            "\u001b[K     |████████████████████████████████| 752.0MB 21kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp==1.0.0rc3) (3.6.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp==1.0.0rc3) (1.4.1)\n",
            "Collecting transformers<2.9.0,>=2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 35.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp==1.0.0rc3) (3.2.5)\n",
            "Collecting conllu==2.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/a8/03/4a952eb39cdc8da80a6a2416252e71784dda6bf9d726ab98065fff2aeb73/conllu-2.3.2-py2.py3-none-any.whl\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp==1.0.0rc3) (2.10.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp==1.0.0rc3) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp==1.0.0rc3) (1.18.4)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 60.0MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/b8/a8588d4010f13716a324f55d23999259bad9db2320f4fe919a66b2f651f3/jsonnet-0.15.0.tar.gz (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 63.8MB/s \n",
            "\u001b[?25hCollecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/01/0c/e4da4191474e27bc41bedab2bf249b27d9261db749f59769d7e7ca8feead/responses-0.10.14-py2.py3-none-any.whl\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting py-rouge==1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0rc3) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0rc3) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0rc3) (0.6.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0rc3) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0rc3) (46.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0rc3) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0rc3) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0rc3) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0rc3) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp==1.0.0rc3) (1.0.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==1.0.0rc3) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==1.0.0rc3) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==1.0.0rc3) (1.16.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp==1.0.0rc3) (0.14.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<=1.5.0,>1.3.1->allennlp==1.0.0rc3) (0.16.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.0.0rc3) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.0.0rc3) (19.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.0.0rc3) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.0.0rc3) (8.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.0.0rc3) (1.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==1.0.0rc3) (1.8.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 59.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<2.9.0,>=2.8.0->allennlp==1.0.0rc3) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<2.9.0,>=2.8.0->allennlp==1.0.0rc3) (2019.12.20)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 53.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp==1.0.0rc3) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==1.0.0rc3) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==1.0.0rc3) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==1.0.0rc3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==1.0.0rc3) (1.24.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp==1.0.0rc3) (3.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->allennlp==1.0.0rc3) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->allennlp==1.0.0rc3) (2.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<2.9.0,>=2.8.0->allennlp==1.0.0rc3) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp==1.0.0rc3) (3.1.0)\n",
            "Building wheels for collected packages: overrides, jsonnet, word2number, sacremoses\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.8.0-cp36-none-any.whl size=5609 sha256=b9af3a15e00136486b81cd134b5042449c58e95f0d3b4fbbe61bf2e135f04283\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/f1/ba/eaf6cd7d284d2f257dc71436ce72d25fd3be5a5813a37794ab\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.15.0-cp36-cp36m-linux_x86_64.whl size=3319809 sha256=99a34e433b45d5398b10695fc144085cd5b0b8d1371a4b7b52fe57cd6426cdee\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/63/2e/da89cfe1ba08550bd7262d5d9c027edc313980c3b85b3b0a38\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5587 sha256=053f9a281118f89208f0ef53e8dcd6a0e6cb88d66d651705edb4a71790563dc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=77ee92181c645982f0a72180b61eb860750b886c94377b90d1e07028ee0b2137\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built overrides jsonnet word2number sacremoses\n",
            "Installing collected packages: overrides, semantic-version, torch, sentencepiece, tokenizers, sacremoses, transformers, conllu, jsonpickle, flaky, tensorboardX, jsonnet, responses, allennlp, word2number, py-rouge, allennlp-models\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "Successfully installed allennlp-1.0.0rc3 allennlp-models-1.0.0rc3 conllu-2.3.2 flaky-3.6.1 jsonnet-0.15.0 jsonpickle-1.4.1 overrides-2.8.0 py-rouge-1.1 responses-0.10.14 sacremoses-0.0.43 semantic-version-2.8.5 sentencepiece-0.1.90 tensorboardX-2.0 tokenizers-0.5.2 torch-1.5.0 transformers-2.8.0 word2number-1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 711853497/711853497 [00:08<00:00, 82055686.05B/s]\n",
            "100%|██████████| 336/336 [00:00<00:00, 868321.72B/s]\n",
            "100%|██████████| 374434792/374434792 [00:08<00:00, 44626117.39B/s]\n",
            "/usr/local/lib/python3.6/dist-packages/allennlp/data/token_indexers/token_characters_indexer.py:61: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcSdi4Sgi5t9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allen_result = predictor.predict(\n",
        "  sentence=english_text\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMukEZRbi3-H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "bec90e6c-648e-448c-81fb-ba19bf284a2e"
      },
      "source": [
        "for i in zip(allen_result['tags'], allen_result['words']):\n",
        "    if (i[0]) != 'O':\n",
        "      print(i)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('U-MISC', 'Indian')\n",
            "('U-ORG', 'PHP')\n",
            "('U-MISC', 'Laravel')\n",
            "('U-MISC', 'Codeigniter')\n",
            "('B-ORG', 'SAP')\n",
            "('L-ORG', 'FICO')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-0fFs2XD8dU",
        "colab_type": "text"
      },
      "source": [
        "##PullEnti \n",
        "C# library for NER in Russian. In 2016 it took first place at the factRuEval-2016 competition. In 2018 the author ported the code to Java and Python.\n",
        "\n",
        "Pluses of PullEnti:\n",
        "\n",
        "Probably better NER quality for the Russian language. Excellent accuracy, normal completeness. 100% best available for Python solution for NER for Russian.\n",
        "Many built-in entities. Not only basic PERSON, ORG, GEO, but also DATE, MONEY, PHONE, ADDRESS and ~10 others.\n",
        "Very detailed parse. For example, for the phrase \"to the head of Greenpeace Ivan Zalesov\" the user will get the position, organization, separate name and surname, everything in normal form.\n",
        "Besides Russian there are English, Ukrainian languages.\n",
        "It does not depend on other libraries, big models, dictionaries for use are not necessary.\n",
        "\n",
        "[link](https://github.com/pullenti/pullenti-wrapper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbzbfY9JESgh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "7a20d02b-4ab2-4b10-cd57-46caade33af4"
      },
      "source": [
        "!pip install pullenti-wrapper\n",
        "!pip install graphviz"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pullenti-wrapper\n",
            "  Downloading https://files.pythonhosted.org/packages/29/81/808722a63eaff077af39976151f20ade5434095e27b905a63f7ebf234065/pullenti_wrapper-0.8.0-py3-none-any.whl\n",
            "Collecting pullenti==3.21\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/7c/9f1f504966c353777a79523477f1e1cb6a260c3d776b9e8f62038b7e6b8e/pullenti-3.21-py3-none-any.whl (15.0MB)\n",
            "\u001b[K     |████████████████████████████████| 15.1MB 326kB/s \n",
            "\u001b[?25hCollecting pullenti-client==0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/61/69cd265e5c140f00f36fb37fb73678a3d7caa5d43dee3d69bcf34a9a0101/pullenti_client-0.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pullenti-client==0.5.0->pullenti-wrapper) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pullenti-client==0.5.0->pullenti-wrapper) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pullenti-client==0.5.0->pullenti-wrapper) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pullenti-client==0.5.0->pullenti-wrapper) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pullenti-client==0.5.0->pullenti-wrapper) (1.24.3)\n",
            "Installing collected packages: pullenti, pullenti-client, pullenti-wrapper\n",
            "Successfully installed pullenti-3.21 pullenti-client-0.5.0 pullenti-wrapper-0.8.0\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.10.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlwza4PVEqhq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "outputId": "55d4adcb-7ad5-4e38-80ef-3cef87aa1df5"
      },
      "source": [
        "from pullenti_wrapper.processor import (\n",
        "    Processor,\n",
        "    MONEY,\n",
        "    URI,\n",
        "    PHONE,\n",
        "    DATE,\n",
        "    KEYWORD,\n",
        "    DEFINITION,\n",
        "    DENOMINATION,\n",
        "    MEASURE,\n",
        "    BANK,\n",
        "    GEO,\n",
        "    ADDRESS,\n",
        "    ORGANIZATION,\n",
        "    PERSON,\n",
        "    MAIL,\n",
        "    TRANSPORT,\n",
        "    DECREE,\n",
        "    INSTRUMENT,\n",
        "    TITLEPAGE,\n",
        "    BOOKLINK,\n",
        "    BUSINESS,\n",
        "    NAMEDENTITY,\n",
        "    WEAPON,\n",
        ")\n",
        "\n",
        "processor = Processor([PERSON, ORGANIZATION, GEO, DATE, MONEY])\n",
        "text = russian_text\n",
        "result = processor(text)\n",
        "result.graph"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph(...)"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"580pt\" height=\"700pt\"\n viewBox=\"0.00 0.00 579.59 700.47\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 696.4706)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-696.4706 575.5913,-696.4706 575.5913,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"499.9697,-485.0566 452.9697,-485.0566 452.9697,-466.0566 499.9697,-466.0566 499.9697,-485.0566\"/>\n<text text-anchor=\"middle\" x=\"476.4697\" y=\"-473.0566\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">MONEY</text>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"460.1101,-448.0266 425.1101,-448.0266 425.1101,-429.0266 460.1101,-429.0266 460.1101,-448.0266\"/>\n<text text-anchor=\"middle\" x=\"442.6101\" y=\"-436.0266\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">RUB</text>\n</g>\n<!-- 0&#45;&gt;8 -->\n<g id=\"edge20\" class=\"edge\">\n<title>0&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M467.5734,-465.8274C463.2628,-461.1132 458.0476,-455.4096 453.5011,-450.4374\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"454.1291,-449.5682 451.3298,-448.0628 452.5793,-450.9853 454.1291,-449.5682\"/>\n<text text-anchor=\"middle\" x=\"439.0373\" y=\"-451.7324\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">CURRENCY</text>\n</g>\n<!-- 26 -->\n<g id=\"node27\" class=\"node\">\n<title>26</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"528.7395,-519.8396 487.7395,-519.8396 487.7395,-500.8396 528.7395,-500.8396 528.7395,-519.8396\"/>\n<text text-anchor=\"middle\" x=\"508.2395\" y=\"-507.8396\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">8000</text>\n</g>\n<!-- 0&#45;&gt;26 -->\n<g id=\"edge31\" class=\"edge\">\n<title>0&#45;&gt;26</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M485.1509,-485.0612C488.8712,-489.1344 493.2604,-493.9398 497.2074,-498.2611\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"496.549,-499.0973 499.3476,-500.6043 498.0996,-497.6811 496.549,-499.0973\"/>\n<text text-anchor=\"middle\" x=\"477.6791\" y=\"-494.2612\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">VALUE</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"428.7607,-142.747 399.7607,-142.747 399.7607,-123.747 428.7607,-123.747 428.7607,-142.747\"/>\n<text text-anchor=\"middle\" x=\"414.2607\" y=\"-130.747\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">РФ</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"451,-551 392,-551 392,-532 451,-532 451,-551\"/>\n<text text-anchor=\"middle\" x=\"421.5\" y=\"-539\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">Finance</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"473.6579,-183.2198 438.6579,-183.2198 438.6579,-164.2198 473.6579,-164.2198 473.6579,-183.2198\"/>\n<text text-anchor=\"middle\" x=\"456.1579\" y=\"-171.2198\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">GEO</text>\n</g>\n<!-- 3&#45;&gt;1 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M446.0158,-163.9225C440.149,-158.2552 432.725,-151.0836 426.5307,-145.0999\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"427.1967,-144.2833 424.3094,-142.9542 425.7376,-145.7937 427.1967,-144.2833\"/>\n<text text-anchor=\"middle\" x=\"425.2732\" y=\"-157.1112\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">NAME</text>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"523.3696,-131 440.3696,-131 440.3696,-112 523.3696,-112 523.3696,-131\"/>\n<text text-anchor=\"middle\" x=\"481.8696\" y=\"-119\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">государство</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge2\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M460.8738,-164.1419C465.0781,-155.6031 471.2555,-143.0569 475.8221,-133.7824\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"476.8004,-134.1725 477.1836,-131.0172 474.9164,-133.2448 476.8004,-134.1725\"/>\n<text text-anchor=\"middle\" x=\"457.348\" y=\"-151.5622\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">TYPE</text>\n</g>\n<!-- 33 -->\n<g id=\"node34\" class=\"node\">\n<title>33</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"479.9115,-240.742 450.9115,-240.742 450.9115,-221.742 479.9115,-221.742 479.9115,-240.742\"/>\n<text text-anchor=\"middle\" x=\"465.4115\" y=\"-228.742\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">RU</text>\n</g>\n<!-- 3&#45;&gt;33 -->\n<g id=\"edge33\" class=\"edge\">\n<title>3&#45;&gt;33</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M457.7276,-183.4777C459.2791,-193.1217 461.6606,-207.9256 463.3608,-218.4946\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"462.3517,-218.8331 463.8649,-221.6282 464.425,-218.4995 462.3517,-218.8331\"/>\n<text text-anchor=\"middle\" x=\"476.5442\" y=\"-203.5861\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">ALPHA2</text>\n</g>\n<!-- 36 -->\n<g id=\"node37\" class=\"node\">\n<title>36</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"540.19,-192.0893 487.19,-192.0893 487.19,-173.0893 540.19,-173.0893 540.19,-192.0893\"/>\n<text text-anchor=\"middle\" x=\"513.69\" y=\"-180.0893\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">РОССИЯ</text>\n</g>\n<!-- 3&#45;&gt;36 -->\n<g id=\"edge39\" class=\"edge\">\n<title>3&#45;&gt;36</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M473.732,-176.4292C477.0366,-176.9386 480.5745,-177.4841 484.1287,-178.032\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"483.9883,-179.0727 487.1132,-178.4921 484.3083,-176.9972 483.9883,-179.0727\"/>\n<text text-anchor=\"middle\" x=\"484.4304\" y=\"-170.8306\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">NAME</text>\n</g>\n<!-- 45 -->\n<g id=\"node46\" class=\"node\">\n<title>45</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"473,-210.2198 336,-210.2198 336,-191.2198 473,-191.2198 473,-210.2198\"/>\n<text text-anchor=\"middle\" x=\"404.5\" y=\"-198.2198\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">РОССИЙСКАЯ ФЕДЕРАЦИЯ</text>\n</g>\n<!-- 3&#45;&gt;45 -->\n<g id=\"edge37\" class=\"edge\">\n<title>3&#45;&gt;45</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M438.3802,-183.0117C434.3016,-185.1435 429.9253,-187.4308 425.7178,-189.6299\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"425.01,-188.8151 422.8376,-191.1353 425.9828,-190.6762 425.01,-188.8151\"/>\n<text text-anchor=\"middle\" x=\"421.049\" y=\"-179.9208\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">NAME</text>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"23,-171.8088 0,-171.8088 0,-152.8088 23,-152.8088 23,-171.8088\"/>\n<text text-anchor=\"middle\" x=\"11.5\" y=\"-159.8088\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">1</text>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"103.8726,-19 62.8726,-19 62.8726,0 103.8726,0 103.8726,-19\"/>\n<text text-anchor=\"middle\" x=\"83.3726\" y=\"-7\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">1970</text>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"139.9587,-489.5939 80.9587,-489.5939 80.9587,-470.5939 139.9587,-470.5939 139.9587,-489.5939\"/>\n<text text-anchor=\"middle\" x=\"110.4587\" y=\"-477.5939\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">СОБЯНИН</text>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"373.6418,-607.8722 332.6418,-607.8722 332.6418,-588.8722 373.6418,-588.8722 373.6418,-607.8722\"/>\n<text text-anchor=\"middle\" x=\"353.1418\" y=\"-595.8722\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">DATE</text>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"368.328,-651.3229 327.328,-651.3229 327.328,-632.3229 368.328,-632.3229 368.328,-651.3229\"/>\n<text text-anchor=\"middle\" x=\"347.828\" y=\"-639.3229\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">DATE</text>\n</g>\n<!-- 9&#45;&gt;15 -->\n<g id=\"edge34\" class=\"edge\">\n<title>9&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M351.9625,-608.0153C351.2058,-614.2032 350.2145,-622.3087 349.3923,-629.0318\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"348.326,-629.1014 349.004,-632.2067 350.4105,-629.3564 348.326,-629.1014\"/>\n<text text-anchor=\"middle\" x=\"334.6774\" y=\"-621.1235\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">HIGHER</text>\n</g>\n<!-- 31 -->\n<g id=\"node32\" class=\"node\">\n<title>31</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"331,-579 308,-579 308,-560 331,-560 331,-579\"/>\n<text text-anchor=\"middle\" x=\"319.5\" y=\"-567\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">5</text>\n</g>\n<!-- 9&#45;&gt;31 -->\n<g id=\"edge23\" class=\"edge\">\n<title>9&#45;&gt;31</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M341.7522,-588.5974C339.0121,-586.2457 336.0565,-583.7092 333.2225,-581.2769\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"333.8383,-580.4218 330.8779,-579.2647 332.4706,-582.0154 333.8383,-580.4218\"/>\n<text text-anchor=\"middle\" x=\"349.4874\" y=\"-578.5372\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">DAYOFWEEK</text>\n</g>\n<!-- 40 -->\n<g id=\"node41\" class=\"node\">\n<title>40</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"404.6418,-588.7569 381.6418,-588.7569 381.6418,-569.7569 404.6418,-569.7569 404.6418,-588.7569\"/>\n<text text-anchor=\"middle\" x=\"393.1418\" y=\"-576.7569\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">1</text>\n</g>\n<!-- 9&#45;&gt;40 -->\n<g id=\"edge28\" class=\"edge\">\n<title>9&#45;&gt;40</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M373.1418,-588.8146C374.9875,-587.9325 376.8332,-587.0505 378.6161,-586.1985\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"379.1028,-587.1297 381.3569,-584.8887 378.1973,-585.2349 379.1028,-587.1297\"/>\n<text text-anchor=\"middle\" x=\"383.879\" y=\"-590.1065\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">DAY</text>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"250.3517,-295.2672 197.3517,-295.2672 197.3517,-276.2672 250.3517,-276.2672 250.3517,-295.2672\"/>\n<text text-anchor=\"middle\" x=\"223.8517\" y=\"-283.2672\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">PERSON</text>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"301.6898,-346.4744 230.6898,-346.4744 230.6898,-327.4744 301.6898,-327.4744 301.6898,-346.4744\"/>\n<text text-anchor=\"middle\" x=\"266.1898\" y=\"-334.4744\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">КОСТАНТИП</text>\n</g>\n<!-- 10&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>10&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M232.0149,-295.6405C238.8092,-303.858 248.5217,-315.6052 255.8573,-324.4774\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"255.3307,-325.4884 258.0516,-327.1314 256.9492,-324.1503 255.3307,-325.4884\"/>\n<text text-anchor=\"middle\" x=\"219.9361\" y=\"-312.6589\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">FIRSTNAME</text>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"369.7106,-302.1984 268.7106,-302.1984 268.7106,-283.1984 369.7106,-283.1984 369.7106,-302.1984\"/>\n<text text-anchor=\"middle\" x=\"319.2106\" y=\"-290.1984\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">PERSONPROPERTY</text>\n</g>\n<!-- 10&#45;&gt;16 -->\n<g id=\"edge35\" class=\"edge\">\n<title>10&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M250.4145,-287.6979C255.1608,-288.0429 260.2569,-288.4133 265.4509,-288.7909\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"265.627,-289.8564 268.6953,-289.0267 265.7793,-287.7619 265.627,-289.8564\"/>\n<text text-anchor=\"middle\" x=\"269.9327\" y=\"-281.8444\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">ATTRIBUTE</text>\n</g>\n<!-- 21 -->\n<g id=\"node22\" class=\"node\">\n<title>21</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"317.6033,-254.4836 264.6033,-254.4836 264.6033,-235.4836 317.6033,-235.4836 317.6033,-254.4836\"/>\n<text text-anchor=\"middle\" x=\"291.1033\" y=\"-242.4836\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">ИВАНОВ</text>\n</g>\n<!-- 10&#45;&gt;21 -->\n<g id=\"edge22\" class=\"edge\">\n<title>10&#45;&gt;21</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M239.7894,-276.102C249.6723,-270.1087 262.405,-262.3872 272.7144,-256.1352\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"273.2593,-257.0328 275.28,-254.5794 272.1703,-255.2372 273.2593,-257.0328\"/>\n<text text-anchor=\"middle\" x=\"234.7519\" y=\"-268.7186\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">LASTNAME</text>\n</g>\n<!-- 25 -->\n<g id=\"node26\" class=\"node\">\n<title>25</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"217.7849,-381.6421 176.7849,-381.6421 176.7849,-362.6421 217.7849,-362.6421 217.7849,-381.6421\"/>\n<text text-anchor=\"middle\" x=\"197.2849\" y=\"-369.6421\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">MALE</text>\n</g>\n<!-- 10&#45;&gt;25 -->\n<g id=\"edge38\" class=\"edge\">\n<title>10&#45;&gt;25</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M220.9071,-295.3408C216.0797,-311.0359 206.4847,-342.2314 201.1281,-359.647\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"200.1211,-359.3494 200.2427,-362.5255 202.1283,-359.9668 200.1211,-359.3494\"/>\n<text text-anchor=\"middle\" x=\"203.0176\" y=\"-330.0939\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">SEX</text>\n</g>\n<!-- 41 -->\n<g id=\"node42\" class=\"node\">\n<title>41</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"246.6102,-235.2589 169.6102,-235.2589 169.6102,-216.2589 246.6102,-216.2589 246.6102,-235.2589\"/>\n<text text-anchor=\"middle\" x=\"208.1102\" y=\"-223.2589\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">КОСТАНТИПА</text>\n</g>\n<!-- 10&#45;&gt;41 -->\n<g id=\"edge27\" class=\"edge\">\n<title>10&#45;&gt;41</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M221.3226,-276.1258C218.6534,-265.9505 214.4323,-249.8594 211.4816,-238.611\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"212.4623,-238.2111 210.6854,-235.5757 210.431,-238.744 212.4623,-238.2111\"/>\n<text text-anchor=\"middle\" x=\"192.4021\" y=\"-250.9684\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">FIRSTNAME</text>\n</g>\n<!-- 42 -->\n<g id=\"node43\" class=\"node\">\n<title>42</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"293.4798,-214.9563 228.4798,-214.9563 228.4798,-195.9563 293.4798,-195.9563 293.4798,-214.9563\"/>\n<text text-anchor=\"middle\" x=\"260.9798\" y=\"-202.9563\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">ПЕТРОВИЧ</text>\n</g>\n<!-- 10&#45;&gt;42 -->\n<g id=\"edge29\" class=\"edge\">\n<title>10&#45;&gt;42</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M228.2566,-276.2391C234.9556,-261.7486 247.7111,-234.1576 255.1641,-218.0361\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"256.2748,-218.1357 256.5806,-214.972 254.3686,-217.2544 256.2748,-218.1357\"/>\n<text text-anchor=\"middle\" x=\"215.2103\" y=\"-240.7376\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">MIDDLENAME</text>\n</g>\n<!-- 43 -->\n<g id=\"node44\" class=\"node\">\n<title>43</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"159.7238,-252.5617 118.7238,-252.5617 118.7238,-233.5617 159.7238,-233.5617 159.7238,-252.5617\"/>\n<text text-anchor=\"middle\" x=\"139.2238\" y=\"-240.5617\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">DATE</text>\n</g>\n<!-- 10&#45;&gt;43 -->\n<g id=\"edge30\" class=\"edge\">\n<title>10&#45;&gt;43</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M204.6483,-276.0766C191.6816,-269.5333 174.5382,-260.8822 161.0531,-254.0773\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"161.2111,-252.9809 158.0597,-252.5667 160.2649,-254.8557 161.2111,-252.9809\"/>\n<text text-anchor=\"middle\" x=\"171.8507\" y=\"-267.677\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">BORN</text>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"571.5913,-569.3924 524.5913,-569.3924 524.5913,-550.3924 571.5913,-550.3924 571.5913,-569.3924\"/>\n<text text-anchor=\"middle\" x=\"548.0913\" y=\"-557.3924\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">ВТБ24</text>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"481.3483,-400.2116 434.3483,-400.2116 434.3483,-381.2116 481.3483,-381.2116 481.3483,-400.2116\"/>\n<text text-anchor=\"middle\" x=\"457.8483\" y=\"-388.2116\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">MONEY</text>\n</g>\n<!-- 13&#45;&gt;8 -->\n<g id=\"edge6\" class=\"edge\">\n<title>13&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M454.7646,-400.3877C452.4513,-407.6464 449.261,-417.6571 446.7294,-425.601\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"445.5998,-425.6877 445.6892,-428.8649 447.6006,-426.3254 445.5998,-425.6877\"/>\n<text text-anchor=\"middle\" x=\"429.247\" y=\"-415.5943\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">CURRENCY</text>\n</g>\n<!-- 38 -->\n<g id=\"node39\" class=\"node\">\n<title>38</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"515.5877,-355 426.5877,-355 426.5877,-336 515.5877,-336 515.5877,-355\"/>\n<text text-anchor=\"middle\" x=\"471.0877\" y=\"-343\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">110000000000</text>\n</g>\n<!-- 13&#45;&gt;38 -->\n<g id=\"edge25\" class=\"edge\">\n<title>13&#45;&gt;38</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M460.656,-381.1234C462.622,-374.4099 465.2683,-365.3728 467.4036,-358.0808\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"468.4485,-358.2486 468.284,-355.0744 466.4332,-357.6584 468.4485,-358.2486\"/>\n<text text-anchor=\"middle\" x=\"450.5298\" y=\"-372.2021\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">VALUE</text>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"417.1101,-458.8748 370.1101,-458.8748 370.1101,-439.8748 417.1101,-439.8748 417.1101,-458.8748\"/>\n<text text-anchor=\"middle\" x=\"393.6101\" y=\"-446.8748\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">MONEY</text>\n</g>\n<!-- 14&#45;&gt;8 -->\n<g id=\"edge9\" class=\"edge\">\n<title>14&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M417.2489,-444.1414C418.7357,-443.8122 420.2266,-443.4821 421.6999,-443.156\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"422.393,-444.078 425.0951,-442.4043 421.939,-442.0277 422.393,-444.078\"/>\n<text text-anchor=\"middle\" x=\"397.9744\" y=\"-437.2487\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">CURRENCY</text>\n</g>\n<!-- 44 -->\n<g id=\"node45\" class=\"node\">\n<title>44</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"359,-468.567 336,-468.567 336,-449.567 359,-449.567 359,-468.567\"/>\n<text text-anchor=\"middle\" x=\"347.5\" y=\"-456.567\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">1</text>\n</g>\n<!-- 14&#45;&gt;44 -->\n<g id=\"edge26\" class=\"edge\">\n<title>14&#45;&gt;44</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M370.0147,-454.3345C367.4555,-454.8724 364.9052,-455.4085 362.49,-455.9161\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"362.012,-454.9436 359.2922,-456.5883 362.444,-456.9987 362.012,-454.9436\"/>\n<text text-anchor=\"middle\" x=\"379.7524\" y=\"-457.7253\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">VALUE</text>\n</g>\n<!-- 39 -->\n<g id=\"node40\" class=\"node\">\n<title>39</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"352.3629,-692.4706 329.3629,-692.4706 329.3629,-673.4706 352.3629,-673.4706 352.3629,-692.4706\"/>\n<text text-anchor=\"middle\" x=\"340.8629\" y=\"-680.4706\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">5</text>\n</g>\n<!-- 15&#45;&gt;39 -->\n<g id=\"edge21\" class=\"edge\">\n<title>15&#45;&gt;39</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M346.2126,-651.3664C345.2767,-656.8951 344.0855,-663.9326 343.0648,-669.9627\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"341.9834,-670.0599 342.518,-673.1931 344.054,-670.4104 341.9834,-670.0599\"/>\n<text text-anchor=\"middle\" x=\"331.1387\" y=\"-663.2645\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">MONTH</text>\n</g>\n<!-- 29 -->\n<g id=\"node30\" class=\"node\">\n<title>29</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"497.8971,-285.4961 300.8971,-285.4961 300.8971,-266.4961 497.8971,-266.4961 497.8971,-285.4961\"/>\n<text text-anchor=\"middle\" x=\"399.3971\" y=\"-273.4961\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">индивидуальный предприниматель</text>\n</g>\n<!-- 16&#45;&gt;29 -->\n<g id=\"edge14\" class=\"edge\">\n<title>16&#45;&gt;29</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M364.9254,-283.1763C364.9958,-283.1617 365.0661,-283.147 365.1364,-283.1324\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"362.6247,-284.7293 365.3472,-283.0885 362.1957,-282.6734 362.6247,-284.7293\"/>\n<text text-anchor=\"middle\" x=\"376.0309\" y=\"-285.7544\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">NAME</text>\n</g>\n<!-- 17 -->\n<g id=\"node18\" class=\"node\">\n<title>17</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"524.0393,-596.3924 435.0393,-596.3924 435.0393,-577.3924 524.0393,-577.3924 524.0393,-596.3924\"/>\n<text text-anchor=\"middle\" x=\"479.5393\" y=\"-584.3924\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">ORGANIZATION</text>\n</g>\n<!-- 17&#45;&gt;2 -->\n<g id=\"edge24\" class=\"edge\">\n<title>17&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M467.2307,-577.2659C458.1507,-570.1644 445.7596,-560.4734 436.166,-552.9702\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"436.8009,-552.1338 433.7909,-551.1127 435.5072,-553.788 436.8009,-552.1338\"/>\n<text text-anchor=\"middle\" x=\"433.1983\" y=\"-567.718\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">PROFILE</text>\n</g>\n<!-- 17&#45;&gt;11 -->\n<g id=\"edge40\" class=\"edge\">\n<title>17&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M503.9012,-577.2972C509.5896,-575.0568 515.6766,-572.6593 521.458,-570.3823\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"521.862,-571.3517 524.2685,-569.2753 521.0924,-569.3978 521.862,-571.3517\"/>\n<text text-anchor=\"middle\" x=\"501.6796\" y=\"-567.4397\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">NAME</text>\n</g>\n<!-- 20 -->\n<g id=\"node21\" class=\"node\">\n<title>20</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"489.4652,-669.3211 448.4652,-669.3211 448.4652,-650.3211 489.4652,-650.3211 489.4652,-669.3211\"/>\n<text text-anchor=\"middle\" x=\"468.9652\" y=\"-657.3211\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">банк</text>\n</g>\n<!-- 17&#45;&gt;20 -->\n<g id=\"edge11\" class=\"edge\">\n<title>17&#45;&gt;20</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M478.1569,-596.4266C476.2833,-609.3488 472.9316,-632.4657 470.8347,-646.9273\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"469.7731,-646.9322 470.3817,-650.0518 471.8514,-647.2336 469.7731,-646.9322\"/>\n<text text-anchor=\"middle\" x=\"463.4958\" y=\"-624.2769\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">TYPE</text>\n</g>\n<!-- 18 -->\n<g id=\"node19\" class=\"node\">\n<title>18</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"114.7775,-94.3735 73.7775,-94.3735 73.7775,-75.3735 114.7775,-75.3735 114.7775,-94.3735\"/>\n<text text-anchor=\"middle\" x=\"94.2775\" y=\"-82.3735\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">DATE</text>\n</g>\n<!-- 18&#45;&gt;6 -->\n<g id=\"edge36\" class=\"edge\">\n<title>18&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M92.8964,-75.3274C90.9569,-61.9221 87.4207,-37.4802 85.2458,-22.4472\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"86.2458,-22.0254 84.777,-19.2067 84.1674,-22.3261 86.2458,-22.0254\"/>\n<text text-anchor=\"middle\" x=\"78.0711\" y=\"-51.4873\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">YEAR</text>\n</g>\n<!-- 19 -->\n<g id=\"node20\" class=\"node\">\n<title>19</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"304.2652,-107.0724 269.2652,-107.0724 269.2652,-88.0724 304.2652,-88.0724 304.2652,-107.0724\"/>\n<text text-anchor=\"middle\" x=\"286.7652\" y=\"-95.0724\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">GEO</text>\n</g>\n<!-- 30 -->\n<g id=\"node31\" class=\"node\">\n<title>30</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"360.9059,-134.0724 253.9059,-134.0724 253.9059,-115.0724 360.9059,-115.0724 360.9059,-134.0724\"/>\n<text text-anchor=\"middle\" x=\"307.4059\" y=\"-122.0724\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">САНКТ&#45;ПЕТЕРБУРГ</text>\n</g>\n<!-- 19&#45;&gt;30 -->\n<g id=\"edge19\" class=\"edge\">\n<title>19&#45;&gt;30</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M294.217,-107.3201C295.3717,-108.8306 296.5785,-110.4092 297.773,-111.9716\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"297.2396,-113.0028 299.8958,-114.7484 298.9079,-111.7274 297.2396,-113.0028\"/>\n<text text-anchor=\"middle\" x=\"306.995\" y=\"-103.2458\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">NAME</text>\n</g>\n<!-- 32 -->\n<g id=\"node33\" class=\"node\">\n<title>32</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"299,-75 252,-75 252,-56 299,-56 299,-75\"/>\n<text text-anchor=\"middle\" x=\"275.5\" y=\"-63\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">город</text>\n</g>\n<!-- 19&#45;&gt;32 -->\n<g id=\"edge16\" class=\"edge\">\n<title>19&#45;&gt;32</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M283.3241,-87.7754C282.2759,-84.7911 281.1077,-81.4652 280.0004,-78.3127\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"280.905,-77.7196 278.9201,-75.2371 278.9237,-78.4155 280.905,-77.7196\"/>\n<text text-anchor=\"middle\" x=\"270.6622\" y=\"-76.644\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">TYPE</text>\n</g>\n<!-- 22 -->\n<g id=\"node23\" class=\"node\">\n<title>22</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"81.0892,-286.0177 52.0892,-286.0177 52.0892,-267.0177 81.0892,-267.0177 81.0892,-286.0177\"/>\n<text text-anchor=\"middle\" x=\"66.5892\" y=\"-274.0177\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">10</text>\n</g>\n<!-- 23 -->\n<g id=\"node24\" class=\"node\">\n<title>23</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"292.589,-509.2284 239.589,-509.2284 239.589,-490.2284 292.589,-490.2284 292.589,-509.2284\"/>\n<text text-anchor=\"middle\" x=\"266.089\" y=\"-497.2284\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">СЕРГЕЙ</text>\n</g>\n<!-- 24 -->\n<g id=\"node25\" class=\"node\">\n<title>24</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"115.1005,-176.2079 74.1005,-176.2079 74.1005,-157.2079 115.1005,-157.2079 115.1005,-176.2079\"/>\n<text text-anchor=\"middle\" x=\"94.6005\" y=\"-164.2079\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">DATE</text>\n</g>\n<!-- 24&#45;&gt;5 -->\n<g id=\"edge7\" class=\"edge\">\n<title>24&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M74.0588,-165.6205C59.3484,-164.8417 39.8704,-163.8106 26.5814,-163.1071\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"26.3942,-162.0458 23.3428,-162.9357 26.2831,-164.1429 26.3942,-162.0458\"/>\n<text text-anchor=\"middle\" x=\"36.8201\" y=\"-166.9638\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">MONTH</text>\n</g>\n<!-- 24&#45;&gt;18 -->\n<g id=\"edge8\" class=\"edge\">\n<title>24&#45;&gt;18</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M94.5622,-156.999C94.5039,-142.2337 94.3929,-114.1193 94.3281,-97.6919\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"95.3777,-97.5655 94.3157,-94.5697 93.2777,-97.5739 95.3777,-97.5655\"/>\n<text text-anchor=\"middle\" x=\"78.4451\" y=\"-129.9455\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">HIGHER</text>\n</g>\n<!-- 27 -->\n<g id=\"node28\" class=\"node\">\n<title>27</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"80.2652,-387.0724 45.2652,-387.0724 45.2652,-368.0724 80.2652,-368.0724 80.2652,-387.0724\"/>\n<text text-anchor=\"middle\" x=\"62.7652\" y=\"-375.0724\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">GEO</text>\n</g>\n<!-- 28 -->\n<g id=\"node29\" class=\"node\">\n<title>28</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"109.9059,-414.0724 56.9059,-414.0724 56.9059,-395.0724 109.9059,-395.0724 109.9059,-414.0724\"/>\n<text text-anchor=\"middle\" x=\"83.4059\" y=\"-402.0724\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">МОСКВА</text>\n</g>\n<!-- 27&#45;&gt;28 -->\n<g id=\"edge1\" class=\"edge\">\n<title>27&#45;&gt;28</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M70.217,-387.3201C71.3717,-388.8306 72.5785,-390.4092 73.773,-391.9716\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"73.2396,-393.0028 75.8958,-394.7484 74.9079,-391.7274 73.2396,-393.0028\"/>\n<text text-anchor=\"middle\" x=\"60.995\" y=\"-392.2458\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">NAME</text>\n</g>\n<!-- 37 -->\n<g id=\"node38\" class=\"node\">\n<title>37</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"75,-355 28,-355 28,-336 75,-336 75,-355\"/>\n<text text-anchor=\"middle\" x=\"51.5\" y=\"-343\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">город</text>\n</g>\n<!-- 27&#45;&gt;37 -->\n<g id=\"edge5\" class=\"edge\">\n<title>27&#45;&gt;37</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M59.3241,-367.7754C58.2759,-364.7911 57.1077,-361.4652 56.0004,-358.3127\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"56.905,-357.7196 54.9201,-355.2371 54.9237,-358.4155 56.905,-357.7196\"/>\n<text text-anchor=\"middle\" x=\"68.6622\" y=\"-356.644\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">TYPE</text>\n</g>\n<!-- 34 -->\n<g id=\"node35\" class=\"node\">\n<title>34</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"233.2684,-546.3812 132.2684,-546.3812 132.2684,-527.3812 233.2684,-527.3812 233.2684,-546.3812\"/>\n<text text-anchor=\"middle\" x=\"182.7684\" y=\"-534.3812\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">PERSONPROPERTY</text>\n</g>\n<!-- 46 -->\n<g id=\"node47\" class=\"node\">\n<title>46</title>\n<polygon fill=\"#eeeeee\" stroke=\"transparent\" points=\"205.3683,-621.0484 128.3683,-621.0484 128.3683,-602.0484 205.3683,-602.0484 205.3683,-621.0484\"/>\n<text text-anchor=\"middle\" x=\"166.8683\" y=\"-609.0484\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">мэр города</text>\n</g>\n<!-- 34&#45;&gt;46 -->\n<g id=\"edge13\" class=\"edge\">\n<title>34&#45;&gt;46</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M180.6897,-546.6426C177.8411,-560.0199 172.7201,-584.0681 169.575,-598.8375\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"168.4951,-598.8677 168.8972,-602.0206 170.5491,-599.3051 168.4951,-598.8677\"/>\n<text text-anchor=\"middle\" x=\"164.1324\" y=\"-575.34\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">NAME</text>\n</g>\n<!-- 35 -->\n<g id=\"node36\" class=\"node\">\n<title>35</title>\n<polygon fill=\"#aec7e8\" stroke=\"transparent\" points=\"218.1121,-467.0025 165.1121,-467.0025 165.1121,-448.0025 218.1121,-448.0025 218.1121,-467.0025\"/>\n<text text-anchor=\"middle\" x=\"191.6121\" y=\"-455.0025\" font-family=\"sans\" font-size=\"10.00\" fill=\"#000000\">PERSON</text>\n</g>\n<!-- 35&#45;&gt;7 -->\n<g id=\"edge32\" class=\"edge\">\n<title>35&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M165.0394,-464.8998C158.0824,-466.8365 150.4994,-468.9474 143.2595,-470.9629\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"142.789,-470.0038 140.1805,-471.82 143.3522,-472.0269 142.789,-470.0038\"/>\n<text text-anchor=\"middle\" x=\"132.6494\" y=\"-461.5313\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">LASTNAME</text>\n</g>\n<!-- 35&#45;&gt;23 -->\n<g id=\"edge3\" class=\"edge\">\n<title>35&#45;&gt;23</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M208.512,-467.0842C219.8436,-473.5089 234.7997,-481.9885 246.6286,-488.6951\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"246.1283,-489.6184 249.2559,-490.1846 247.1641,-487.7916 246.1283,-489.6184\"/>\n<text text-anchor=\"middle\" x=\"203.5703\" y=\"-480.4896\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">FIRSTNAME</text>\n</g>\n<!-- 35&#45;&gt;25 -->\n<g id=\"edge17\" class=\"edge\">\n<title>35&#45;&gt;25</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M192.2628,-447.7106C193.2887,-432.274 195.2824,-402.2734 196.4251,-385.0797\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"197.4904,-384.8828 196.6417,-381.8198 195.395,-384.7435 197.4904,-384.8828\"/>\n<text text-anchor=\"middle\" x=\"186.344\" y=\"-418.9951\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">SEX</text>\n</g>\n<!-- 35&#45;&gt;34 -->\n<g id=\"edge10\" class=\"edge\">\n<title>35&#45;&gt;34</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M190.5277,-467.2359C188.9364,-481.5186 185.9666,-508.1752 184.1985,-524.0449\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"183.1504,-523.9703 183.8617,-527.0682 185.2375,-524.2029 183.1504,-523.9703\"/>\n<text text-anchor=\"middle\" x=\"163.3631\" y=\"-498.2404\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">ATTRIBUTE</text>\n</g>\n<!-- 43&#45;&gt;22 -->\n<g id=\"edge15\" class=\"edge\">\n<title>43&#45;&gt;22</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M118.6039,-252.5593C107.7162,-257.5743 94.4977,-263.6628 84.1109,-268.4471\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"83.5462,-267.5511 81.2606,-269.7599 84.4248,-269.4585 83.5462,-267.5511\"/>\n<text text-anchor=\"middle\" x=\"93.3574\" y=\"-263.1032\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">DAY</text>\n</g>\n<!-- 43&#45;&gt;24 -->\n<g id=\"edge18\" class=\"edge\">\n<title>43&#45;&gt;24</title>\n<path fill=\"none\" stroke=\"#c0c0c0\" d=\"M133.5722,-233.3914C125.5692,-219.6976 110.922,-194.6351 102.0425,-179.4416\"/>\n<polygon fill=\"#c0c0c0\" stroke=\"#c0c0c0\" points=\"102.7675,-178.6011 100.3471,-176.5408 100.9544,-179.6607 102.7675,-178.6011\"/>\n<text text-anchor=\"middle\" x=\"101.8074\" y=\"-209.0165\" font-family=\"sans\" font-size=\"8.00\" fill=\"#c0c0c0\">HIGHER</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neNsoOteHLja",
        "colab_type": "text"
      },
      "source": [
        "##Natasha\n",
        "![alt text](https://raw.githubusercontent.com/natasha/natasha-logos/master/natasha.svg)\n",
        "\n",
        "\n",
        "Natasha solves basic NLP tasks for Russian language: tokenization, sentence segmentation, word embedding, morphology tagging, lemmatization, phrase normalization, syntax parsing, NER tagging, fact extraction. Quality on every task is similar or better then current SOTAs for Russian language on news articles, see evaluation section. Natasha is not a research project, underlying technologies are built for production. We pay attention to model size, RAM usage and performance. Models run on CPU, use Numpy for inference.\n",
        "\n",
        "Natasha integrates libraries from Natasha project under one convenient API:\n",
        "\n",
        "Razdel — token, sentence segmentation for Russian\n",
        "\n",
        "Navec — compact Russian embeddings\n",
        "\n",
        "Slovnet — modern deep-learning techniques for Russian NLP, compact models for Russian morphology, syntax, NER.\n",
        "\n",
        "Yargy — rule-based fact extraction similar to Tomita parser.\n",
        "\n",
        "Ipymarkup — NLP visualizations for NER and syntax markups.\n",
        "\n",
        "\n",
        "[link](https://github.com/natasha/natasha)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks2xst6sHKsc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "63cab9f9-ad91-4c8e-db92-2cc55b2c8f6f"
      },
      "source": [
        "!pip install natasha"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: natasha in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: yargy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.14.0)\n",
            "Requirement already satisfied: navec>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.8)\n",
            "Requirement already satisfied: slovnet>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.3.0)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: ipymarkup>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from navec>=0.9.0->natasha) (1.18.4)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (2.4.393442.3710985)\n",
            "Requirement already satisfied: intervaltree>=3 in /usr/local/lib/python3.6/dist-packages (from ipymarkup>=0.8.0->natasha) (3.0.2)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izC9kj8rHeC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    \n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "    \n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")\n",
        "\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "names_extractor = NamesExtractor(morph_vocab)\n",
        "\n",
        "doc = Doc(russian_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiywhyAVarGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "23f3375f-05d8-457e-d6d5-74dbbdc020c4"
      },
      "source": [
        "doc.segment(segmenter)\n",
        "doc.tag_ner(ner_tagger)\n",
        "display(doc.spans[:5])\n",
        "doc.ner.print()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DocSpan(start=7, stop=13, type='LOC', text='Москвы', tokens=[...]),\n",
              " DocSpan(start=205, stop=223, type='LOC', text='Алтуфьевское шоссе', tokens=[...]),\n",
              " DocSpan(start=319, stop=322, type='ORG', text='МПЗ', tokens=[...]),\n",
              " DocSpan(start=324, stop=339, type='PER', text='Подпоручик Киже', tokens=[...]),\n",
              " DocSpan(start=417, stop=422, type='ORG', text='ВТБ24', tokens=[...])]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Власти Москвы выделили 110 млрд рублей на поддержку населения, системы\n",
            "       LOC───                                                         \n",
            " здравоохранения и городского хозяйства. Об этом сообщается на сайте \n",
            "мэра столицы https://www.sobyanin.ru/ в пятницу, 1 мая. По адресу \n",
            "Алтуфьевское шоссе д.51 (основной вид разрешенного использования: \n",
            "LOC───────────────                                                \n",
            "производственная деятельность, склады) размещен МПЗ? Подпоручик Киже \n",
            "                                                ORG  PER──────────── \n",
            "управляя автомобилем ВАЗ2107 перевозил автомат АК47 с целью ограбления\n",
            " банка ВТБ24, как следует из записей. \n",
            "       ORG──                          \n",
            "Взыскать c индивидуального предпринимателя Иванова Костантипа \n",
            "                                           PER────────────────\n",
            "Петровича дата рождения 10 января 1970 года, проживающего по адресу \n",
            "─────────                                                           \n",
            "город Санкт-Петербург, ул. Крузенштерна, дом 5/1А 8 000 (восемь тысяч)\n",
            "      LOC────────────      PER─────────                               \n",
            " рублей 00 копеей госпошлины в пользу бюджета РФ Жители требуют \n",
            "                                              LO                \n",
            "незамедлительной остановки МПЗ и его вывода из района. Решение было \n",
            "принято по поручению мэра города Сергея Собянина в связи с \n",
            "                                 PER────────────           \n",
            "ограничениями из-за коронавируса.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNjJksbbeEjI",
        "colab_type": "text"
      },
      "source": [
        "##ner-d\n",
        "\n",
        "ner-d is a Python module for Named Entity Recognition (NER). Named entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
        "\n",
        "Gives you easy use with a single main function and flexibility for selecting language models. It automatically downloads models if not downloaded before and links on system and finds the entities from given text block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMHUclJreFKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "5f239d24-0ae1-443c-ec99-78d75221b3a0"
      },
      "source": [
        "!pip install ner-d"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ner-d\n",
            "  Downloading https://files.pythonhosted.org/packages/67/57/e407b539030c810cccc98db72d3e6b6789ad44037869baf6597ac4ace671/ner_d-0.3.0-py3-none-any.whl\n",
            "Collecting spacy==2.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/05/e82c888a36f24608664b56abe737f4428410d370791f6112fb3e9b4a4a81/spacy-2.2.2-cp36-cp36m-manylinux1_x86_64.whl (10.3MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3MB 3.0MB/s \n",
            "\u001b[?25hCollecting thinc<7.4.0,>=7.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/59/6bb553bc9a5f072d3cd479fc939fea0f6f682892f1f5cff98de5c9b615bb/thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2->ner-d) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2->ner-d) (1.6.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2->ner-d) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2->ner-d) (46.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2->ner-d) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2->ner-d) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2->ner-d) (1.18.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2->ner-d) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2->ner-d) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2->ner-d) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2->ner-d) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.2->ner-d) (4.41.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy==2.2.2->ner-d) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2->ner-d) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2->ner-d) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2->ner-d) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2->ner-d) (2020.4.5.1)\n",
            "Installing collected packages: thinc, spacy, ner-d\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed ner-d-0.3.0 spacy-2.2.2 thinc-7.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2gBM2_IeVwd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90ab144e-71f2-4ed4-ec6c-112153156517"
      },
      "source": [
        "from nerd import ner\n",
        "\n",
        "doc_nerd_d = ner.name(english_text)\n",
        "text_label = [(X.text, X.label_) for X in doc_nerd_d]\n",
        "print(text_label)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('7 days', 'DATE'), ('Indian', 'NORP'), ('PHP', 'ORG'), ('Laravel', 'GPE'), ('daily 4 hours', 'DATE'), ('Monday 27th Jan.', 'DATE'), ('Need SAP FICO', 'PERSON'), ('6 months', 'DATE'), ('10 days', 'DATE'), ('noon', 'TIME'), ('tomorrow', 'DATE')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}